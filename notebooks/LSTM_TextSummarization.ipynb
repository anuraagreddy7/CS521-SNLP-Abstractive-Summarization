{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hEEkd5Qos20x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb60OegVKFvA",
        "outputId": "eb84b1d4-35f3-4896-b724-67d713938f00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/anuraagreddy/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c61IdvLUtvKh"
      },
      "outputs": [],
      "source": [
        "# read data from the csv file (from the location it is stored)\n",
        "Data = pd.read_csv(\"/Users/anuraagreddy/Desktop/MS(UIC)/Spring '25/CS 521 (SNLP)/Stat NLP Project/CS521_TextSummarization_Project/wikihowAll_Dataset.csv\")\n",
        "Data = Data.astype(str)\n",
        "rows, columns = Data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "JS1b4wJtw_xG",
        "outputId": "72daa5be-6262-4b41-c444-99d31a8f29aa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\nKeep related supplies in the same area.,\\nMa...</td>\n",
              "      <td>How to Be an Organized Artist1</td>\n",
              "      <td>If you're a photographer, keep all the necess...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\nCreate a sketch in the NeoPopRealist manner ...</td>\n",
              "      <td>How to Create a Neopoprealist Art Work</td>\n",
              "      <td>See the image for how this drawing develops s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\nGet a bachelor‚Äôs degree.,\\nEnroll in a stu...</td>\n",
              "      <td>How to Be a Visual Effects Artist1</td>\n",
              "      <td>It is possible to become a VFX artist without...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\nStart with some experience or interest in ar...</td>\n",
              "      <td>How to Become an Art Investor</td>\n",
              "      <td>The best art investors do their research on t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\nKeep your reference materials, sketches, art...</td>\n",
              "      <td>How to Be an Organized Artist2</td>\n",
              "      <td>As you start planning for a project or work, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1494</th>\n",
              "      <td>\\nScout your yard.,\\nCreate a grip.,\\nWrap the...</td>\n",
              "      <td>How to Build a Squirrel House2</td>\n",
              "      <td>Plan to spend a day watching your yard to wit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1495</th>\n",
              "      <td>\\nAsk for pedigree papers.,\\nGet a DNA test.,\\...</td>\n",
              "      <td>How to Identify Birman Cats2</td>\n",
              "      <td>Cats like the Birman that are more exclusive ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>\\nDo a search on the Internet for \"Birman\" cat...</td>\n",
              "      <td>How to Identify Birman Cats3</td>\n",
              "      <td>This can help you find any breeders in your a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>\\nTake good care of your Bengal cat.,\\nPlay wi...</td>\n",
              "      <td>How to Keep a Bengal Cat Happy</td>\n",
              "      <td>Make sure everything is done correctly, so yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>\\nDecide which spotted cat you're looking at.,...</td>\n",
              "      <td>How to Identify Feline Species by Fur</td>\n",
              "      <td>If you're trying to work out which spotted gr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1499 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               headline  \\\n",
              "0     \\nKeep related supplies in the same area.,\\nMa...   \n",
              "1     \\nCreate a sketch in the NeoPopRealist manner ...   \n",
              "2     \\nGet a bachelor‚Äôs degree.,\\nEnroll in a stu...   \n",
              "3     \\nStart with some experience or interest in ar...   \n",
              "4     \\nKeep your reference materials, sketches, art...   \n",
              "...                                                 ...   \n",
              "1494  \\nScout your yard.,\\nCreate a grip.,\\nWrap the...   \n",
              "1495  \\nAsk for pedigree papers.,\\nGet a DNA test.,\\...   \n",
              "1496  \\nDo a search on the Internet for \"Birman\" cat...   \n",
              "1497  \\nTake good care of your Bengal cat.,\\nPlay wi...   \n",
              "1498  \\nDecide which spotted cat you're looking at.,...   \n",
              "\n",
              "                                       title  \\\n",
              "0             How to Be an Organized Artist1   \n",
              "1     How to Create a Neopoprealist Art Work   \n",
              "2         How to Be a Visual Effects Artist1   \n",
              "3              How to Become an Art Investor   \n",
              "4             How to Be an Organized Artist2   \n",
              "...                                      ...   \n",
              "1494          How to Build a Squirrel House2   \n",
              "1495            How to Identify Birman Cats2   \n",
              "1496            How to Identify Birman Cats3   \n",
              "1497          How to Keep a Bengal Cat Happy   \n",
              "1498   How to Identify Feline Species by Fur   \n",
              "\n",
              "                                                   text  \n",
              "0      If you're a photographer, keep all the necess...  \n",
              "1      See the image for how this drawing develops s...  \n",
              "2      It is possible to become a VFX artist without...  \n",
              "3      The best art investors do their research on t...  \n",
              "4      As you start planning for a project or work, ...  \n",
              "...                                                 ...  \n",
              "1494   Plan to spend a day watching your yard to wit...  \n",
              "1495   Cats like the Birman that are more exclusive ...  \n",
              "1496   This can help you find any breeders in your a...  \n",
              "1497   Make sure everything is done correctly, so yo...  \n",
              "1498   If you're trying to work out which spotted gr...  \n",
              "\n",
              "[1499 rows x 3 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uWTmu1FbCUuS"
      },
      "outputs": [],
      "source": [
        "# Filtering rows where 'text' and 'headline' columns are not null\n",
        "df = Data[(Data['text'].notna()) & (Data['headline'].notna())]\n",
        "\n",
        "# Removing rows where the 'text' column has 'nan' as its value\n",
        "df = df[df['text'] != 'nan']\n",
        "\n",
        "# Dropping duplicate entries based on the 'text' column\n",
        "df = df.drop_duplicates(subset='text', inplace=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yW3xqCcEyhDb"
      },
      "outputs": [],
      "source": [
        "# Set of stopwords\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text_input(input_text, mode):\n",
        "    modified_text = input_text.lower()\n",
        "    modified_text = BeautifulSoup(modified_text, \"lxml\").get_text()\n",
        "    modified_text = re.sub(r'\\([^)]*\\)', '', modified_text)\n",
        "    modified_text = modified_text.replace('\"', '')\n",
        "    modified_text = ' '.join([contraction_mapping.get(word, word) for word in modified_text.split()])\n",
        "    modified_text = re.sub(r\"'s\\b\", \"\", modified_text)\n",
        "    modified_text = re.sub(\"[^a-zA-Z]\", \" \", modified_text)\n",
        "    modified_text = re.sub('[m]{2,}', 'mm', modified_text)\n",
        "\n",
        "    if mode == 0:\n",
        "        modified_text = re.sub(r'\\.', ' . ', modified_text)\n",
        "        words_list = [word for word in modified_text.split() if word not in english_stopwords]\n",
        "    else:\n",
        "        words_list = modified_text.split()\n",
        "\n",
        "    # Filter out single-character words\n",
        "    longer_words = [word for word in words_list if len(word) > 1]\n",
        "    return \" \".join(longer_words).strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RUozoMqQ1Pes"
      },
      "outputs": [],
      "source": [
        "# Flat contraction mapping\n",
        "contraction_mapping = {\n",
        "    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
        "    \"I'm\": \"I am\", \"I've\": \"I have\",\n",
        "    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
        "    \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n",
        "    \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n",
        "    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n",
        "    \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "    \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n",
        "    \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
        "    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n",
        "    \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
        "    \"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\", \"isn't\": \"is not\", \"mustn't\": \"must not\", \"shan't\": \"shall not\",\n",
        "    \"shouldn't\": \"should not\", \"wasn't\": \"was not\", \"weren't\": \"were not\", \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\", \"'cause\": \"because\", \"o'clock\": \"of the clock\", \"ma'am\": \"madam\", \"let's\": \"let us\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1fmqZ6fb60np"
      },
      "outputs": [],
      "source": [
        "cleaned_texts_list = []\n",
        "for t in df['text']:\n",
        "    cleaned_texts_list.append(clean_text_input(t, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EVI9HLowAf62"
      },
      "outputs": [],
      "source": [
        "#call the function\n",
        "clean_summary = []\n",
        "for t in df['headline']:\n",
        "    clean_summary.append(clean_text_input(t,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "a50C4-IU9Gb7"
      },
      "outputs": [],
      "source": [
        "df['text']=cleaned_texts_list\n",
        "df['headline']=clean_summary\n",
        "\n",
        "df.replace('', np.nan, inplace=True)\n",
        "df.dropna(axis=0,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC7uHaPoNcH5"
      },
      "source": [
        "Updating the df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fGQUB8kOyaO"
      },
      "source": [
        "**Analyzing the sequence distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "h306jOy2N6ui",
        "outputId": "a971dddb-8c94-484d-ba29-ac2930cff0ab"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+0AAAJJCAYAAADfvdZfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB+SElEQVR4nOzdeVxV1f7/8feRGUQUUJBCxXLG2TS1BFMw5/SWlTlm5c0mUrNsErslSWXeq2nZoN7MbFJvg5lUaHnFRA1L62qDWpZEKYkD4gHW7w9/nG9HQDlwgI2+no8Hj0d77bX3/uwPJ9f+sPdex2aMMQIAAAAAAJZTq7oDAAAAAAAAJaNoBwAAAADAoijaAQAAAACwKIp2AAAAAAAsiqIdAAAAAACLomgHAAAAAMCiKNoBAAAAALAoinYAAAAAACyKoh0AAAAAAIuiaAeA/+/tt9+WzWbTG2+8UWxd+/btZbPZ9NFHHxVbd8kll6hTp06VGtv69etls9m0fv36MvX/6quvNH78eEVFRcnX11e1a9dWp06dlJycrMOHD1dqrGW1fPlyzZ07t8z9Y2NjZbPZZLPZVKtWLQUGBurSSy/Vddddp7fffluFhYXFtmnSpInGjRvnUlybNm1SYmKi/vzzT5e2O/NYRb+zt99+26X9nM2JEyeUmJhY4udgyZIlstls2rdvn9uOV1axsbGKjY11y76aNGni+D2f7WfJkiVuOd7ZclrdEhMTZbPZztlv3Lhxql27dqnra9eu7fL/B+5y5mdj3759xX5/1fnZBYCawLO6AwAAqygqClNTU3X99dc72g8fPqyvv/5aAQEBSk1NVb9+/RzrDhw4oB9//FGTJ0+ujpBL9OKLL2rSpElq0aKF7rvvPrVu3Vp2u11bt27V888/r7S0NK1ataq6w9Ty5cu1c+dOJSQklHmbpk2b6rXXXpMkHT9+XHv37tXq1at13XXX6corr9R7772noKAgR/9Vq1apTp06LsW1adMmzZw5U+PGjVPdunXLvF15juWqEydOaObMmZJUrEgeOHCg0tLS1LBhw0qNoSQLFixw275WrVqlvLw8x/JLL72kl19+WWvXrnX63V5yySVuOd7ZcoqqUZ2fXQCoCSjaAeD/Cw0NVXR0dLE7bhs2bJCnp6cmTJig1NRUp3VFy717967w8XNzc+Xn51ehfaSlpen2229XXFycVq9eLR8fH8e6uLg4TZkyRWvXrq1oqNXGz89Pl19+uVPbLbfcosWLF+vmm2/Wbbfd5vSkRMeOHSs9pqLfW1Uc62zq16+v+vXrV8uxW7du7bZ9nZnHos9r586dFRoa6rbjwDqq87MLADUBj8cDwF/07t1bu3fv1sGDBx1t69ev12WXXaYBAwZo27ZtOnr0qNM6Dw8PXXnllZKkkydPavr06YqKipK3t7cuuugi3XHHHcUetW7SpIkGDRqklStXqmPHjvL19XXc7fvf//6nq6++Wv7+/goNDdXf//53p2OezaxZs2Sz2bRo0SKngr2It7e3hgwZ4lguLCxUcnKyWrZsKR8fHzVo0EBjxozRgQMHisVb0uO1Zz76WvRI+Ouvv66HHnpIERERqlOnjvr27avdu3c7bffBBx9o//79To88l9f48eM1YMAAvfXWW9q/f3+pcRcWFurxxx9XixYt5Ofnp7p166pdu3b65z//Ken048j33XefJCkqKsoRV9Efcs72eystRydPntTkyZMVHh4uPz8/xcTE6MsvvzxrHouMGzdOTZo0kXT6seKiwmbmzJmO2IqOWdojxq+88orat28vX19fBQcHa9iwYfr222+LHad27dr6/vvvNWDAANWuXVuRkZGaMmWK013v0pT2CPTTTz+tOXPmKCoqSrVr11b37t21efPmc+7vXIwxWrBggTp06CA/Pz/Vq1dP1157rX788UdHnxUrVshms2n+/PlO286YMUMeHh5KSUk5Z05LcvLkSU2ZMkUdOnRQUFCQgoOD1b17d/3nP/8p1tdms+nOO+/Uq6++qlatWsnf31/t27fX+++/X6zvBx98oA4dOsjHx0dRUVF6+umny5mdssnJydHUqVOd/q1KSEjQ8ePHnfo999xz6tWrlxo0aKCAgAC1bdtWycnJstvtTv2MMUpOTlbjxo3l6+urTp066cMPPyxTLCV9dmNjYxUdHa309HRdeeWV8vf3V9OmTfXkk08WexWmrOfy1ltvqVu3bgoKCnLs7+abb3YhawBQPbjTDgB/0bt3b/3rX//S+vXrdeONN0o6fTd90KBB6tmzp2w2mz7//HMNGDDAsa5Tp04KCgqSMUbXXHONPvnkE02fPl1XXnmlvvrqK82YMUNpaWlKS0tzKqS3b9+ub7/9Vg8//LCioqIUEBCg3377TTExMfLy8tKCBQsUFham1157TXfeeec5Yy8oKNCnn36qzp07KzIyskzne/vtt2vRokW68847NWjQIO3bt0+PPPKI1q9fr+3bt5f7zuaDDz6onj176qWXXlJOTo7uv/9+DR48WN9++608PDy0YMEC3Xbbbfrhhx/c9qj+kCFDtGbNGn3++edq3LhxiX2Sk5OVmJiohx9+WL169ZLdbtf//vc/xx9VbrnlFh0+fFjz5s3TypUrHY/r/vVOckm/t3PlolOnTnrppZd05MgRJSYmKjY2Vl9++aWaNm1a5vNr2LCh1q5dq6uvvloTJkzQLbfcIklnvUOZlJSkBx98UDfeeKOSkpJ06NAhJSYmqnv37kpPT1ezZs0cfe12u4YMGaIJEyZoypQp+uyzz/SPf/xDQUFBevTRR8sc518999xzatmypWPugkceeUQDBgzQ3r17nR51d9XEiRO1ZMkS3X333Zo9e7YOHz6sxx57TD169NCOHTsUFhamG264QRs2bNCUKVN0+eWXq0uXLvr000/1+OOP68EHH1RcXJzy8vJczmleXp4OHz6sqVOn6qKLLtKpU6f08ccfa/jw4Vq8eLHGjBnj1P+DDz5Qenq6HnvsMdWuXVvJyckaNmyYdu/e7fj9f/LJJxo6dKi6d++uFStWqKCgQMnJyfrtt99cykt+fn6Z+p04cUIxMTE6cOCAHnzwQbVr1067du3So48+qq+//loff/yx449oP/zwg0aOHOkoiHfs2KEnnnhC//vf//TKK6849jlz5kzNnDlTEyZM0LXXXquff/5Zt956qwoKCtSiRQuXzqNIZmambrrpJk2ZMkUzZszQqlWrNH36dEVERDjyXNZzSUtL0/XXX6/rr79eiYmJ8vX11f79+/Xpp5+WKzYAqFIGAOBw+PBhU6tWLXPbbbcZY4z5448/jM1mM2vXrjXGGNO1a1czdepUY4wxP/30k5Fkpk2bZowxZu3atUaSSU5OdtrnG2+8YSSZRYsWOdoaN25sPDw8zO7du5363n///cZms5mMjAyn9ri4OCPJpKamlhp7ZmamkWRuuOGGMp3rt99+aySZSZMmObV/8cUXRpJ58MEHneIdO3ZssX3ExMSYmJgYx3JqaqqRZAYMGODU78033zSSTFpamqNt4MCBpnHjxmWKtehYbdq0KXX9hx9+aCSZ2bNnlxr3oEGDTIcOHc56nKeeespIMnv37i22rrTfW0nHKspFp06dTGFhoaN93759xsvLy9xyyy1O5/bXPBYZO3asU45+//13I8nMmDGjWN/Fixc7xZ2dnW38/PyK/S5++ukn4+PjY0aOHOl0HEnmzTffdOo7YMAA06JFi2LHOtOZ8e/du9dIMm3btjX5+fmO9i1bthhJ5vXXXz/nPovMmDHDSDK///67McaYtLQ0I8k888wzTv1+/vln4+fn5/j/0RhjTp48aTp27GiioqLMN998Y8LCwkxMTIxTTGfLaVnk5+cbu91uJkyYYDp27Oi0TpIJCwszOTk5jrbMzExTq1Ytk5SU5Gjr1q2biYiIMLm5uY62nJwcExwcbMpyqVb0+zvbz18/m0lJSaZWrVomPT3daT9vv/22kWTWrFlT4nEKCgqM3W43//73v42Hh4c5fPiwMeb0Z83X19cMGzbMqf9///tfI6nEz8bixYsdbWd+do05/ZmSZL744gunfbZu3dr069fP5XN5+umnjSTz559/lpxEALAwHo8HgL+oV6+e2rdv73gcesOGDfLw8FDPnj0lSTExMY732M98n73ojs2Zj9Zed911CggI0CeffOLU3q5dOzVv3typLTU1VW3atFH79u2d2keOHFnxkztDUfxnxtu1a1e1atWqWLyu+Osj+NLpc5Xk9Oi6uxljztmna9eu2rFjhyZNmqSPPvpIOTk5Lh+npN/b2YwcOdLp0f/GjRurR48exeZHcLe0tDTl5uYW+/1GRkbqqquuKvb7tdlsGjx4sFNbu3btKvQ7GzhwoDw8PJz2J1Xsc/D+++/LZrNp1KhRys/Pd/yEh4c7/b8rST4+PnrzzTd16NAhderUScYYvf76604xlcdbb72lnj17qnbt2vL09JSXl5defvnlYq8dSKf/fQgMDHQsh4WFqUGDBo4cHD9+XOnp6Ro+fLh8fX0d/QIDA4v9Ps7Gz89P6enpJf6cOVfG+++/r+joaHXo0MEph/369Sv2LRVffvmlhgwZopCQEHl4eMjLy0tjxoxRQUGB9uzZI+n0Z+3kyZO66aabnI7To0ePUp96KYvw8HB17drVqe3Mz2RZz+Wyyy6TJI0YMUJvvvmmfvnll3LHBQBVjaIdAM7Qu3dv7dmzR7/++qtSU1PVuXNnx9cpFb2PfOTIEaWmpsrT01NXXHGFJOnQoUPy9PQs9mitzWZTeHi4Dh065NRe0kzJhw4dUnh4eLH2ktrOFBoaKn9/f+3du7dM51kUT0lxREREFIvXFSEhIU7LRa8F5Obmlnuf51J0IR8REVFqn+nTp+vpp5/W5s2b1b9/f4WEhKhPnz7aunVrmY/j6gzXpf0+K5LfsnD19+vv7+9UNEqnf28nT54sdwyV8Tn47bffZIxRWFiYvLy8nH42b96sP/74w6n/pZdeqiuvvNJRVFZ0hvKVK1dqxIgRuuiii7Rs2TKlpaUpPT1dN998c4m5OjMH0uk8FOUgOztbhYWF5f7/vkitWrXUpUuXEn9q1XK+3Pvtt9/01VdfFctfYGCgjDGOHP7000+68sor9csvv+if//ynPv/8c6Wnp+u5556T9H+/x6LPUkXP4Uznyp0r59KrVy+tXr1a+fn5GjNmjC6++GJFR0fr9ddfL3d8AFBVeKcdAM7Qu3dvzZkzR+vXr9f69esd769LchTon332mWOCuqKCPiQkRPn5+fr999+dCndjjDIzMx13eoqUNPFaSEiIMjMzi7WX1HYmDw8P9enTRx9++KEOHDigiy+++Kz9iy6IDx48WKzvr7/+6vQ+u6+vb4kTkv3xxx+WmdH73Xfflc1mU69evUrt4+npqcmTJ2vy5Mn6888/9fHHH+vBBx9Uv3799PPPP8vf3/+cx3F1wrzSfp9/LUh8fX115MiRYv3OLEBd8dff75nO/P3WJKGhoY65JUqabPHMtpdeekkffPCBunbtqvnz5+v6669Xt27dyn38ZcuWKSoqSm+88YbTZ6EsE/aVpF69erLZbOX+/748QkND5efn5/RO+pnrJWn16tU6fvy4Vq5c6XTHPCMjw6l/0WettHMomkyxMpT1XCRp6NChGjp0qPLy8rR582YlJSVp5MiRatKkibp3715pMQJARXGnHQDO0KtXL3l4eOjtt9/Wrl27nGbFDgoKUocOHbR06VLt27fP6ave+vTpI+n0Rf1fvfPOOzp+/Lhj/dn07t1bu3bt0o4dO5zaly9fXqbYp0+fLmOMbr31Vp06darYervdrvfee0+SdNVVV5UYb3p6ur799luneJs0aaKvvvrKqd+ePXucZoR31Zl3zCpi8eLF+vDDD3XjjTeqUaNGZdqmbt26uvbaa3XHHXfo8OHDjpmr3f1UwOuvv+706P7+/fu1adMmp89VkyZNtGfPHqfC79ChQ9q0aZPTvlyJrXv37vLz8yv2+z1w4IA+/fTTMn0erWjQoEEyxuiXX34p8a5y27ZtHX2//vpr3X333RozZow+//xztWvXTtdff72ys7MdfVz9fdtsNnl7ezsV7JmZmSXOHl8WAQEB6tq1q1auXOl0p/7o0aOO/1fdbdCgQfrhhx8UEhJSYg6Liuyic/zrH0KMMXrxxRed9nf55ZfL19dXr732mlP7pk2bKvWVGFfO5a98fHwUExOj2bNnS1Kxb3MAAKvhTjsAnKFOnTrq1KmTVq9erVq1ajneZy8SExPjmA37r0V7XFyc+vXrp/vvv185OTnq2bOnY/b4jh07avTo0ec8dkJCgl555RUNHDhQjz/+uGP2+P/9739lir179+5auHChJk2apM6dO+v2229XmzZtZLfb9eWXX2rRokWKjo7W4MGD1aJFC912222aN2+eatWqpf79+ztmj4+MjNS9997r2O/o0aM1atQoTZo0SX/729+0f/9+JScnV+i7ldu2bauVK1dq4cKF6ty5s+Px3rPJzc11fGVYbm6ufvzxR61evVrvv/++YmJi9Pzzz591+8GDBys6OlpdunRR/fr1tX//fs2dO1eNGzd2zKReVPT985//1NixY+Xl5aUWLVo4vZfsiqysLA0bNky33nqrjhw5ohkzZsjX11fTp0939Bk9erReeOEFjRo1SrfeeqsOHTqk5ORk1alTx2lfgYGBaty4sf7zn/+oT58+Cg4OVmhoaImFSd26dfXII4/owQcf1JgxY3TjjTfq0KFDmjlzpnx9fTVjxoxynU9169mzp2677TaNHz9eW7duVa9evRQQEKCDBw9q48aNatu2rW6//XYdP35cI0aMUFRUlBYsWCBvb2+9+eab6tSpk8aPH6/Vq1dLci2nkhxf+Tdp0iTHLOn/+Mc/1LBhQ3333XflOqd//OMfuvrqqxUXF6cpU6aooKBAs2fPVkBAgA4fPlzOTJUuISFB77zzjnr16qV7771X7dq1U2FhoX766SetW7dOU6ZMUbdu3RQXFydvb2/deOONmjZtmk6ePKmFCxc6/dFDOv20wNSpU/X444/rlltu0XXXXaeff/5ZiYmJFXo83p3n8uijj+rAgQPq06ePLr74Yv3555/65z//KS8vL8XExFRqjABQYdU1Ax4AWNm0adOMJNOlS5di61avXm0kGW9vb3P8+HGndbm5ueb+++83jRs3Nl5eXqZhw4bm9ttvN9nZ2U79GjdubAYOHFjisb/55hsTFxdnfH19TXBwsJkwYYL5z3/+c87Z4/8qIyPDjB071jRq1Mh4e3ubgIAA07FjR/Poo4+arKwsR7+CggIze/Zs07x5c+Pl5WVCQ0PNqFGjzM8//+y0v8LCQpOcnGyaNm1qfH19TZcuXcynn35a6uzxb731ltP2Jc0YffjwYXPttdeaunXrGpvNds5Zsotmky76CQgIME2bNjXXXnuteeutt0xBQUGxbc6c0f2ZZ54xPXr0MKGhocbb29s0atTITJgwwezbt89pu+nTp5uIiAhTq1Ytp7yf7fdW2uzxr776qrn77rtN/fr1jY+Pj7nyyivN1q1bi22/dOlS06pVK+Pr62tat25t3njjjWKzxxtjzMcff2w6duxofHx8nGYFL2kGbmOMeemll0y7du2Mt7e3CQoKMkOHDjW7du1y6jN27FgTEBBQLKaimdvPpbTZ45966qlifeXiTO1nzh5f5JVXXjHdunUzAQEBxs/Pz1xyySVmzJgxjtyOGjXK+Pv7FzvXt956y0gyzz77rKOttJyW5sknnzRNmjQxPj4+plWrVubFF18sMVeSzB133FFs+5K+jeHdd991/J4aNWpknnzyyTLnv7TfX5GAgIBixzt27Jh5+OGHTYsWLRyfjbZt25p7773XZGZmOvq99957pn379sbX19dcdNFF5r777nN8U8Nf/z0qLCw0SUlJJjIy0nh7e5t27dqZ9957r9TPRllmjy/p2yJK+n+iLOfy/vvvm/79+5uLLrrIeHt7mwYNGpgBAwaYzz//vPTEAoBF2Iwpw3S7AAAAAACgyvFOOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFEU7AAAAAAAWRdEOAAAAAIBFUbQDAAAAAGBRFO0AAAAAAFgURTsAAAAAABZF0Q4AAAAAgEVRtAMAAAAAYFEU7QAAAAAAWBRFOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFEU7AAAAAAAWRdEOAAAAAIBFUbQDAAAAAGBRFO0AAAAAAFgURTsAAAAAABZF0Q4AAAAAgEVRtAMAAAAAYFEU7QAAAAAAWBRFOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtQAXZbLYy/axfv94tx/v111+VmJiojIyMc/bdunWrbDabZs+eXWzd0KFDZbPZ9MILLxRb16dPH4WEhMgY446QS7Rv3z7ZbDYtWbKkTP1//PFH3XnnnWrevLn8/Pzk7++vNm3a6OGHH9Yvv/xSaXG6Ys2aNUpMTKzuMAAAkpYsWSKbzaatW7eWuH7QoEFq0qRJ1Qb1/yUmJspmszm1NWnSROPGjXMsuzpOusvbb78tm82mN954o9i69u3by2az6aOPPiq27pJLLlGnTp0qNbb169e7dE311Vdfafz48YqKipKvr69q166tTp06KTk5WYcPH67UWMtq+fLlmjt3bnWHAYujaAcqKC0tzelnwIAB8vPzK9buroHs119/1cyZM8tUtHfq1ElBQUFKTU11ai8sLNTnn3+ugICAYutOnTqltLQ0xcbGFrugqC7vv/++2rVrp/fff1+33Xab3n//fcd/v/feexo0aFB1hyjpdNE+c+bM6g4DAHAeaNiwodLS0jRw4MAqPW7R+H/m9cHhw4f19ddfl3jtcODAAf3444/q3bt3VYZ6Vi+++KI6d+6s9PR03XfffVq7dq1WrVql6667Ts8//7wmTJhQ3SFKomhH2XhWdwBATXf55Zc7LdevX1+1atUq1l4datWqpV69eik1NVX5+fny9Dz9v/yOHTuUnZ2tqVOn6tVXX3Xa5osvvlBubq5bBt4TJ07I39+/QvvYu3evbrjhBjVv3lypqakKCgpyrLvqqqt09913a9WqVRUNFQAAS/Hx8amWa4nQ0FBFR0cXu5u9YcMGeXp6asKECcWK9qJld1w75Obmys/Pr0L7SEtL0+233664uDitXr1aPj4+jnVxcXGaMmWK1q5dW9FQgSrDnXagCpw6dUqPP/64WrZsKR8fH9WvX1/jx4/X77//7ujz5JNPqlatWnrvvfecth03bpz8/f319ddfa/369brsssskSePHj3c8en+2R7J79+6tY8eOOT0euH79ekVEROiWW27Rb7/9pm+++cZpXdF20um78snJyY7YGzRooDFjxujAgQNOx4mNjVV0dLQ+++wz9ejRQ/7+/rr55pslnX46YMSIEQoMDFRQUJCuv/56ZWZmlil3c+bM0fHjx7VgwQKngr2IzWbT8OHDndpeeeUVtW/fXr6+vgoODtawYcP07bffFos3Nja22P7GjRvn9Lhk0eOJTz/9tObMmaOoqCjVrl1b3bt31+bNm522e+655xwxFf3s27evTOcJAKh+xhgtWLBAHTp0kJ+fn+rVq6drr71WP/74o1O/lJQUDR06VBdffLF8fX116aWXauLEifrjjz+K7fODDz5Qhw4d5OPjo6ioKD399NNliqWkx+OLHqvftWuXbrzxRgUFBSksLEw333yzjhw5Uq5zKUnv3r21e/duHTx40NFWdA0yYMAAbdu2TUePHnVa5+HhoSuvvFKSdPLkSU2fPl1RUVHy9vbWRRddpDvuuEN//vmn03GaNGmiQYMGaeXKlerYsaN8fX0dT6z973//09VXXy1/f3+Fhobq73//u9Mxz2bWrFmy2WxatGiRU8FexNvbW0OGDHEsl/Va58xXGIqceU1R9Bj/66+/roceekgRERGqU6eO+vbtq927dztt98EHH2j//v1O1w7AmSjagUpWWFiooUOH6sknn9TIkSP1wQcf6Mknn1RKSopiY2OVm5srSbr//vvVv39/jR07Vvv375ckLV68WEuXLtW8efPUtm1bderUSYsXL5YkPfzww45H72+55ZZSj19UfP/1r+KpqamKiYlRixYtFB4e7vTX9NTUVNWvX1+tW7eWJN1+++26//77FRcXp3fffVf/+Mc/tHbtWvXo0aPYxcnBgwc1atQojRw5UmvWrNGkSZOUm5urvn37at26dUpKStJbb72l8PBwXX/99WXK37p16xQWFlbmuw1JSUmaMGGC2rRpo5UrV+qf//ynvvrqK3Xv3l3fffddmfZRkueee04pKSmaO3euXnvtNR0/flwDBgxwXCQ98sgjuvbaayU5vzLRsGHDch8TAFBxBQUFys/PL/ZT0rwtEydOVEJCgvr27avVq1drwYIF2rVrl3r06KHffvvN0e+HH35Q9+7dtXDhQq1bt06PPvqovvjiC11xxRWy2+2Ofp988omGDh2qwMBArVixQk899ZTefPNNx1heXn/729/UvHlzvfPOO3rggQe0fPly3XvvveU6l5IUXTuceX0QExOjnj17ymaz6fPPP3daV/RKnjFG11xzjZ5++mmNHj1aH3zwgSZPnqylS5fqqquuUl5entOxtm/frvvuu09333231q5dq7/97W/67bffFBMTo507d2rBggV69dVXdezYMd15553nzE1BQYE+/fRTde7cWZGRkefsL7l2reOKBx98UPv379dLL72kRYsW6bvvvtPgwYNVUFAgSVqwYIF69uyp8PBwp2sHoBgDwK3Gjh1rAgICHMuvv/66kWTeeecdp37p6elGklmwYIGj7Y8//jAXX3yx6dq1q9m+fbvx9/c3o0aNKnG7xYsXlymewsJCExwcbOLj440xxhQUFJi6deua559/3hhjzIgRI8y1115rjDEmLy/P+Pn5mREjRhhjjPn222+NJDNp0iSnfX7xxRdGknnwwQcdbTExMUaS+eSTT5z6Lly40Egy//nPf5zab7311jKdh6+vr7n88svLdK7Z2dnGz8/PDBgwwKn9p59+Mj4+PmbkyJFO8cbExBTbx9ixY03jxo0dy3v37jWSTNu2bU1+fr6jfcuWLUaSef311x1td9xxh+GfVQCwhsWLFxtJZ/3567/3aWlpRpJ55plnnPbz888/Gz8/PzNt2rQSj1NYWGjsdrvZv39/sfGuW7duJiIiwuTm5jracnJyTHBwcLHxonHjxmbs2LGO5aLx56/j5IwZM4wkk5yc7LTtpEmTjK+vryksLKzQuRQ5fPiwqVWrlrntttuMMaevT2w2m1m7dq0xxpiuXbuaqVOnGmNOj7GSHPtcu3ZtiTG+8cYbRpJZtGiR0zl7eHiY3bt3O/W9//77jc1mMxkZGU7tcXFxRpJJTU0tNfbMzEwjydxwww1nPccirlzrnPk7KnLmNUVqaqqRVOx65M033zSSTFpamqNt4MCBTp9DoCTcaQcq2fvvv6+6detq8ODBTn/h79ChQ7G73CEhIXrjjTe0fft29ejRQ40aNdLzzz9foePbbDbFxMTov//9r+x2uzIyMvTnn386HuOKiYnR+vXrZYzR5s2bnd5nL7o7f+ajYF27dlWrVq30ySefOLXXq1dPV111lVNbamqqAgMDnR5Dk6SRI0dW6LxKkpaWptzc3GLxRkZG6qqrrioWrysGDhwoDw8Px3K7du0kyfFUBADAmv79738rPT292M8VV1zh1O/999+XzWbTqFGjnMbr8PBwtW/f3mm8zsrK0t///ndFRkbK09NTXl5eaty4sSQ5Xsc6fvy40tPTNXz4cPn6+jq2DQwM1ODBgyt0TmeOqe3atdPJkyeVlZXl8rmUpF69ek79NmzYIA8PD/Xs2VPS6WuHomuEM99n//TTTyUVv3a47rrrFBAQUGwsbteunZo3b+7UlpqaqjZt2qh9+/ZO7ZVx7eDqtY4rSvo9SVw7wHVMRAdUst9++01//vmnvL29S1x/5mNX3bp1U5s2bbRjxw7dfvvtCggIqHAMvXv31qpVq5Senq60tDSFhYWpRYsWkk4PvH/88Yd27dpVbOA9dOiQJJX4iHdERESxQaekfocOHVJYWFix9vDw8DLF3qhRI+3du7dMfc8Vb0pKSpn2U5KQkBCn5aJ35IpebwAAWFOrVq3UpUuXYu1BQUH6+eefHcu//fabjDEljlmS1LRpU0mnX3uLj4/Xr7/+qkceeURt27ZVQECACgsLdfnllzvGhezsbBUWFpY43pV1DCzNucaksp7L2fTu3Vtz5szRr7/+qtTUVHXu3Fm1a9eWdPra4ZlnntGRI0eUmpoqT09Pxx9BDh06JE9PT9WvX99pfzabTeHh4Y6xukhp1w5RUVHF2suSt9DQUPn7+7vt2qEiBTbXDnAXinagkoWGhiokJKTUWUoDAwOdlmfMmKGvv/5anTt31qOPPqpBgwaVaXA9m7++m5aWlqaYmBjHutatWys0NFSpqalav369GjZs6CjoiwabgwcP6uKLL3ba56+//qrQ0FCntpImTwkJCdGWLVuKtZd1Irp+/fpp3rx52rx58znfa/9rvGc6M15fX99ik/ZIxf+IAgC4MISGhjre1S5p8rKitp07d2rHjh1asmSJxo4d61j//fffO/WvV6+ebDZbieNdWcfA8irruZxNUdG+fv16rV+/XgMGDHCsKyrQP/vsM8cEdUUFfUhIiPLz8/X77787Fe7GGGVmZjom1C1S2rVDefPm4eGhPn366MMPP9SBAweKXb+UdCypbNc6vr6+xd7Jl05fO5x5TQS4E4/HA5Vs0KBBOnTokAoKCtSlS5diP0UFsnR6NtqkpCQ9/PDDSklJccy0furUKUef8vyVtk2bNqpfv74+/fRTff75504znNpsNvXq1Utr167V5s2bnb6upehR92XLljntLz09Xd9++6369OlzzmP37t1bR48e1bvvvuvUvnz58jLFfu+99yogIECTJk0qscg2xji+8q179+7y8/MrFu+BAwf06aefOsXbpEkT7dmzx2nwPXTokDZt2lSmuErCX9ABoOYaNGiQjDH65ZdfShyv27ZtK+n/iswzC98XXnjBaTkgIEBdu3bVypUrdfLkSUf70aNHi31TTHWdy9n06tVLHh4eevvtt7Vr1y6na4egoCB16NBBS5cu1b59+5yuHYrG2jPH4nfeeUfHjx8v87XDrl27tGPHDqf2sl47TJ8+XcYY3XrrrU7XUEXsdrvjd+DKtU6TJk301VdfOfXbs2eP04zwrvLx8eG6AefEnXagkt1www167bXXNGDAAN1zzz3q2rWrvLy8dODAAaWmpmro0KEaNmyYY+b1mJgYzZgxQ7Vq1dIbb7yhXr16adq0aZo7d64k6ZJLLpGfn59ee+01tWrVSrVr11ZERIQiIiJKjcFmsyk2NlZvv/22jDFOd9ql04+5JSQkyBjjNPC2aNFCt912m+bNm6datWqpf//+2rdvnx555BFFRkYWm6m2JGPGjNGzzz6rMWPG6IknnlCzZs20Zs0affTRR2XKX1RUlFasWKHrr79eHTp00J133qmOHTtKkr755hu98sorMsZo2LBhqlu3rh555BE9+OCDGjNmjG688UYdOnRIM2fOlK+vr2bMmOHY7+jRo/XCCy9o1KhRuvXWW3Xo0CElJyerTp06ZYqrJEUXQbNnz1b//v3l4eGhdu3alfpqBADAOnr27KnbbrtN48eP19atW9WrVy8FBATo4MGD2rhxo9q2bavbb79dLVu21CWXXKIHHnhAxhgFBwfrvffeK/EVrH/84x+6+uqrHd8NXlBQoNmzZysgIECHDx+u9nM5mzp16qhTp05avXq1atWq5XifvUhMTIzj2uSv1w5xcXHq16+f7r//fuXk5Khnz5766quvNGPGDHXs2FGjR48+Z/wJCQl65ZVXNHDgQD3++OMKCwvTa6+9pv/9739lOv+imf0nTZqkzp076/bbb1ebNm1kt9v15ZdfatGiRYqOjtbgwYNdutYZPXq0Ro0apUmTJulvf/ub9u/fr+Tk5GKvAriibdu2WrlypRYuXKjOnTurVq1aJb7OgQtc9cx/B5y/zpw93hhj7Ha7efrpp0379u2Nr6+vqV27tmnZsqWZOHGi+e6770x+fr6JiYkxYWFh5uDBg07bPvXUU0aSWbVqlaPt9ddfNy1btjReXl5GkpkxY8Y541qwYIGRZOrXr19sXUZGhmMm3e+++85pXUFBgZk9e7Zp3ry58fLyMqGhoWbUqFHm559/duoXExNj2rRpU+KxDxw4YP72t7+Z2rVrm8DAQPO3v/3NbNq0yaVZ8H/44QczadIkc+mllxofHx/j5+dnWrdubSZPnmz27t3r1Pell14y7dq1M97e3iYoKMgMHTrU7Nq1q9g+ly5dalq1amV8fX1N69atzRtvvFHq7PFPPfVUse3PzH1eXp655ZZbTP369Y3NZjOSisUGAKgaRbPHp6enl7i+tFm7X3nlFdOtWzcTEBBg/Pz8zCWXXGLGjBljtm7d6ujzzTffmLi4OBMYGGjq1atnrrvuOscs6meOye+++65jTGrUqJF58sknHbPA/5Urs8f//vvvJZ7rmWNOWc7lbKZNm2YkmS5duhRbt3r1aiPJeHt7m+PHjzuty83NNffff79p3Lix8fLyMg0bNjS33367yc7OLnbOAwcOLPHYRTn29fU1wcHBZsKECeY///nPOWeP/6uMjAwzduxY06hRI+Pt7W0CAgJMx44dzaOPPmqysrIc/cp6rVNYWGiSk5NN06ZNja+vr+nSpYv59NNPS509/q233nLavqTf6eHDh821115r6tat67h2AM5kM6aEL6kEAAAAAADVjnfaAQAAAACwKIp2AAAAAAAsiqIdAAAAAACLomgHAAAAAMCiKNoBAAAAALAoinYAAAAAACzKs7oDsILCwkL9+uuvCgwMlM1mq+5wAACQMUZHjx5VRESEatXib+wVxVgPALCaso71FO2Sfv31V0VGRlZ3GAAAFPPzzz/r4osvru4wajzGegCAVZ1rrKdolxQYGCjpdLLq1KlToX3Z7XatW7dO8fHx8vLyckd45z1y5jpy5jpy5jpy5hp35ysnJ0eRkZGOMQoVw1hfvciZ68iZ68iZ68iZ69yZs7KO9RTtkuMxuTp16rhlIPf391edOnX44JcROXMdOXMdOXMdOXNNZeWLR7ndg7G+epEz15Ez15Ez15Ez11VGzs411vOSHAAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFEU7AAAAAAAWRdEOAAAAAIBFUbQDAAAAAGBRFO0AAAAAAFgURTsAAAAAABZF0Q4AAAAAgEVRtAMAAAAAYFEU7QAAAAAAWBRFOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAACg3D777DMNHjxYERERstlsWr16tdN6Y4wSExMVEREhPz8/xcbGateuXU598vLydNdddyk0NFQBAQEaMmSIDhw44NQnOztbo0ePVlBQkIKCgjR69Gj9+eeflXx2AABUP4p2AABQbsePH1f79u01f/78EtcnJydrzpw5mj9/vtLT0xUeHq64uDgdPXrU0SchIUGrVq3SihUrtHHjRh07dkyDBg1SQUGBo8/IkSOVkZGhtWvXau3atcrIyNDo0aMr/fwAAKhuntUdAAAAqLn69++v/v37l7jOGKO5c+fqoYce0vDhwyVJS5cuVVhYmJYvX66JEyfqyJEjevnll/Xqq6+qb9++kqRly5YpMjJSH3/8sfr166dvv/1Wa9eu1ebNm9WtWzdJ0osvvqju3btr9+7datGiRdWcLAAA1YCiHQAAVIq9e/cqMzNT8fHxjjYfHx/FxMRo06ZNmjhxorZt2ya73e7UJyIiQtHR0dq0aZP69euntLQ0BQUFOQp2Sbr88ssVFBSkTZs2lVi05+XlKS8vz7Gck5MjSbLb7bLb7RU6r6LtK7qfCwk5cx05cx05cx05c507c1bWfVC0w+1uWJTmUn9PW6Gua1BJwQAAqk1mZqYkKSwszKk9LCxM+/fvd/Tx9vZWvXr1ivUp2j4zM1MNGhQfKBo0aODoc6akpCTNnDmzWPu6devk7+/v+smUICUlxS37uZCQM9eRM9eRM9eRM9e5I2cnTpwoUz+KdgAAUKlsNpvTsjGmWNuZzuxTUv+z7Wf69OmaPHmyYzknJ0eRkZGKj49XnTp1XAm/GLvdrpSUFK36PVj5puzTAy0e17VCx63JinIWFxcnLy+v6g6nRiBnriNnriNnrnNnzoqeAjsXinaclat3zQEAKBIeHi7p9J3yhg0bOtqzsrIcd9/Dw8N16tQpZWdnO91tz8rKUo8ePRx9fvvtt2L7//3334vdxS/i4+MjHx+fYu1eXl5uuzDNN7VcKtq5IHZv/i8U5Mx15Mx15Mx17shZWbdn9ngAAFApoqKiFB4e7vQI4alTp7RhwwZHQd65c2d5eXk59Tl48KB27tzp6NO9e3cdOXJEW7ZscfT54osvdOTIEUcfAADOV9xpBwAA5Xbs2DF9//33juW9e/cqIyNDwcHBatSokRISEjRr1iw1a9ZMzZo106xZs+Tv76+RI0dKkoKCgjRhwgRNmTJFISEhCg4O1tSpU9W2bVvHbPKtWrXS1VdfrVtvvVUvvPCCJOm2227ToEGDmDkeAHDeo2gHAADltnXrVvXu3duxXPQe+dixY7VkyRJNmzZNubm5mjRpkrKzs9WtWzetW7dOgYGBjm2effZZeXp6asSIEcrNzVWfPn20ZMkSeXh4OPq89tpruvvuux2zzA8ZMqTU74YHAOB8QtEOAADKLTY2VsaYUtfbbDYlJiYqMTGx1D6+vr6aN2+e5s2bV2qf4OBgLVu2rCKhAgBQI/FOOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFEU7AAAAAAAWRdEOAAAAAIBFUbQDAAAAAGBR1Vq0f/bZZxo8eLAiIiJks9m0evXqYn2+/fZbDRkyREFBQQoMDNTll1+un376ybE+Ly9Pd911l0JDQxUQEKAhQ4bowIEDVXgWAAAAAABUjmot2o8fP6727dtr/vz5Ja7/4YcfdMUVV6hly5Zav369duzYoUceeUS+vr6OPgkJCVq1apVWrFihjRs36tixYxo0aJAKCgqq6jQAAAAAAKgUntV58P79+6t///6lrn/ooYc0YMAAJScnO9qaNm3q+O8jR47o5Zdf1quvvqq+fftKkpYtW6bIyEh9/PHH6tevX+UFDwAAAABAJavWov1sCgsL9cEHH2jatGnq16+fvvzyS0VFRWn69Om65pprJEnbtm2T3W5XfHy8Y7uIiAhFR0dr06ZNpRbteXl5ysvLcyzn5ORIkux2u+x2e4XiLtq+ovuxCk9bYZUd43zJWVU43z5nVYGcuY6cucbd+SLvAABAsnDRnpWVpWPHjunJJ5/U448/rtmzZ2vt2rUaPny4UlNTFRMTo8zMTHl7e6tevXpO24aFhSkzM7PUfSclJWnmzJnF2tetWyd/f3+3xJ+SkuKW/VS36xpU3bHOl5xVJXLmOnLmOnLmGnfl68SJE27ZDwAAqNksW7QXFp6++zp06FDde++9kqQOHTpo06ZNev755xUTE1PqtsYY2Wy2UtdPnz5dkydPdizn5OQoMjJS8fHxqlOnToXittvtSklJUVxcnLy8vCq0LysYv2RLpR/D01aoYfUPnzc5qwrn2+esKpAz15Ez17g7X0VPgQEAgAubZYv20NBQeXp6qnXr1k7trVq10saNGyVJ4eHhOnXqlLKzs53utmdlZalHjx6l7tvHx0c+Pj7F2r28vNx2YerOfVWnfFN1cxWeLzmrSuTMdeTMdeTMNe7KFzkHAACShb+n3dvbW5dddpl2797t1L5nzx41btxYktS5c2d5eXk5PYp48OBB7dy586xFOwAAAAAANUG13mk/duyYvv/+e8fy3r17lZGRoeDgYDVq1Ej33Xefrr/+evXq1Uu9e/fW2rVr9d5772n9+vWSpKCgIE2YMEFTpkxRSEiIgoODNXXqVLVt29YxmzwAAAAAADVVtRbtW7duVe/evR3LRe+Zjx07VkuWLNGwYcP0/PPPKykpSXfffbdatGihd955R1dccYVjm2effVaenp4aMWKEcnNz1adPHy1ZskQeHh5Vfj4AAAAAALhTtRbtsbGxMsactc/NN9+sm2++udT1vr6+mjdvnubNm+fu8AAAAAAAqFaWfacdAAAAAIALHUU7AAAAAAAWRdEOAAAAAIBFUbQDAAAAAGBRFO0AAAAAAFgURTsAAAAAABZF0Q4AAAAAgEVRtAMAAAAAYFEU7QAAAAAAWBRFOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFEU7AAAAAAAWRdEOAAAAAIBFUbQDAAAAAGBRFO0AAAAAAFgURTsAAAAAABZF0Q4AAAAAgEVRtAMAAAAAYFEU7QAAAAAAWBRFOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFEU7AAAAAAAWRdEOAAAAAIBFUbQDAAAAAGBRFO0AAAAAAFgURTsAAAAAABZF0Q4AAAAAgEVRtAMAAAAAYFEU7QAAAAAAWBRFOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFEU7AAAAAAAW5VndAaBq3bAorbpDAAAAAACUEXfaAQAAAACwKIp2AAAAAAAsiqIdAAAAAACLqtai/bPPPtPgwYMVEREhm82m1atXl9p34sSJstlsmjt3rlN7Xl6e7rrrLoWGhiogIEBDhgzRgQMHKjdwAAAAAACqQLVORHf8+HG1b99e48eP19/+9rdS+61evVpffPGFIiIiiq1LSEjQe++9pxUrVigkJERTpkzRoEGDtG3bNnl4eFRm+HCz8Uu2KN+U/e9IK27rXonRAAAAAED1q9aivX///urfv/9Z+/zyyy+688479dFHH2ngwIFO644cOaKXX35Zr776qvr27StJWrZsmSIjI/Xxxx+rX79+lRY7AAAAAACVzdJf+VZYWKjRo0frvvvuU5s2bYqt37Ztm+x2u+Lj4x1tERERio6O1qZNm0ot2vPy8pSXl+dYzsnJkSTZ7XbZ7fYKxVy0fUX3U1k8bYXVHUIxRTG5GptVc1wVrP45syJy5jpy5hp354u8AwAAyeJF++zZs+Xp6am77767xPWZmZny9vZWvXr1nNrDwsKUmZlZ6n6TkpI0c+bMYu3r1q2Tv79/xYL+/1JSUtyyH3e7rkF1R1C6YfUPu9R/zZo1lRRJzWHVz5mVkTPXkTPXuCtfJ06ccMt+AABAzWbZon3btm365z//qe3bt8tms7m0rTHmrNtMnz5dkydPdizn5OQoMjJS8fHxqlOnTrljlk7fGUlJSVFcXJy8vLwqtK/KMH7JluoOoRhPW6GG1T+sVb8Hu/RO++JxXSsxKmuz+ufMisiZ68iZa9ydr6KnwAAAwIXNskX7559/rqysLDVq1MjRVlBQoClTpmju3Lnat2+fwsPDderUKWVnZzvdbc/KylKPHj1K3bePj498fHyKtXt5ebntwtSd+3InV4riqpZvarkUnxXzW9Ws+jmzMnLmOnLmGnfli5wDAADJwt/TPnr0aH311VfKyMhw/EREROi+++7TRx99JEnq3LmzvLy8nB5FPHjwoHbu3HnWoh0AAAAAgJqgWu+0Hzt2TN9//71jee/evcrIyFBwcLAaNWqkkJAQp/5eXl4KDw9XixYtJElBQUGaMGGCpkyZopCQEAUHB2vq1Klq27atYzZ5AAAAAABqqmot2rdu3arevXs7loveMx87dqyWLFlSpn08++yz8vT01IgRI5Sbm6s+ffpoyZIlfEc7AAAAAKDGq9aiPTY2VsaYMvfft29fsTZfX1/NmzdP8+bNc2NkAAAAAABUP8u+0w4AAAAAwIWOoh0AAAAAAIuiaAcAAAAAwKIo2gEAAAAAsCiKdgAAAAAALIqiHQAAAAAAi6JoBwAAlSY/P18PP/ywoqKi5Ofnp6ZNm+qxxx5TYWGho48xRomJiYqIiJCfn59iY2O1a9cup/3k5eXprrvuUmhoqAICAjRkyBAdOHCgqk8HAIAqR9EOAAAqzezZs/X8889r/vz5+vbbb5WcnKynnnpK8+bNc/RJTk7WnDlzNH/+fKWnpys8PFxxcXE6evSoo09CQoJWrVqlFStWaOPGjTp27JgGDRqkgoKC6jgtAACqjGd1BwAAAM5faWlpGjp0qAYOHChJatKkiV5//XVt3bpV0um77HPnztVDDz2k4cOHS5KWLl2qsLAwLV++XBMnTtSRI0f08ssv69VXX1Xfvn0lScuWLVNkZKQ+/vhj9evXr3pODgCAKkDRDgAAKs0VV1yh559/Xnv27FHz5s21Y8cObdy4UXPnzpUk7d27V5mZmYqPj3ds4+Pjo5iYGG3atEkTJ07Utm3bZLfbnfpEREQoOjpamzZtKrFoz8vLU15enmM5JydHkmS322W32yt0TkXbe9oKz9Gz5O0uREXnfiHnwFXkzHXkzHXkzHXuzFlZ90HRDgAAKs3999+vI0eOqGXLlvLw8FBBQYGeeOIJ3XjjjZKkzMxMSVJYWJjTdmFhYdq/f7+jj7e3t+rVq1esT9H2Z0pKStLMmTOLta9bt07+/v4VPi9JGlb/sEv916xZ45bj1mQpKSnVHUKNQ85cR85cR85c546cnThxokz9KNoBAECleeONN7Rs2TItX75cbdq0UUZGhhISEhQREaGxY8c6+tlsNqftjDHF2s50tj7Tp0/X5MmTHcs5OTmKjIxUfHy86tSpU4EzOn1nJCUlRat+D1a+Kfv0QIvHda3QcWuyopzFxcXJy8urusOpEciZ68iZ68iZ69yZs6KnwM6Foh0AAFSa++67Tw888IBuuOEGSVLbtm21f/9+JSUlaezYsQoPD5d0+m56w4YNHdtlZWU57r6Hh4fr1KlTys7OdrrbnpWVpR49epR4XB8fH/n4+BRr9/LyctuFab6p5VLRzgWxe/N/oSBnriNnriNnrnNHzsq6PbPHAwCASnPixAnVquV8ueHh4eH4yreoqCiFh4c7PWZ46tQpbdiwwVGQd+7cWV5eXk59Dh48qJ07d5ZatAMAcL7gTjsAAKg0gwcP1hNPPKFGjRqpTZs2+vLLLzVnzhzdfPPNkk4/Fp+QkKBZs2apWbNmatasmWbNmiV/f3+NHDlSkhQUFKQJEyZoypQpCgkJUXBwsKZOnaq2bds6ZpMHAOB8RdEOAAAqzbx58/TII49o0qRJysrKUkREhCZOnKhHH33U0WfatGnKzc3VpEmTlJ2drW7dumndunUKDAx09Hn22Wfl6empESNGKDc3V3369NGSJUvk4eFRHacFAECVoWgHAACVJjAwUHPnznV8xVtJbDabEhMTlZiYWGofX19fzZs3T/PmzXN/kAAAWBjvtAMAAAAAYFEU7QAAAAAAWBRFOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFEU7AAAAAAAWRdEOAAAAAIBFUbQDAAAAAGBRFO0AAAAAAFgURTsAAAAAABZF0Q4AAAAAgEVRtAMAAAAAYFEU7QAAAAAAWBRFOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFEU7AAAAAAAWRdEOAAAAAIBFUbQDAAAAAGBRFO0AAAAAAFgURTsAAAAAABZF0Q4AAAAAgEVVa9H+2WefafDgwYqIiJDNZtPq1asd6+x2u+6//361bdtWAQEBioiI0JgxY/Trr7867SMvL0933XWXQkNDFRAQoCFDhujAgQNVfCYAAAAAALhftRbtx48fV/v27TV//vxi606cOKHt27frkUce0fbt27Vy5Urt2bNHQ4YMceqXkJCgVatWacWKFdq4caOOHTumQYMGqaCgoKpOAwAAAACASuFZnQfv37+/+vfvX+K6oKAgpaSkOLXNmzdPXbt21U8//aRGjRrpyJEjevnll/Xqq6+qb9++kqRly5YpMjJSH3/8sfr161fp5wAAAAAAQGWpUe+0HzlyRDabTXXr1pUkbdu2TXa7XfHx8Y4+ERERio6O1qZNm6opSgAAAAAA3KNa77S74uTJk3rggQc0cuRI1alTR5KUmZkpb29v1atXz6lvWFiYMjMzS91XXl6e8vLyHMs5OTmSTr9Hb7fbKxRn0fYV3U9l8bQVVncIxRTF5GpsVs1xVbD658yKyJnryJlr3J0v8g4AAKQaUrTb7XbdcMMNKiws1IIFC87Z3xgjm81W6vqkpCTNnDmzWPu6devk7+9foViLnPlov1Vc16C6IyjdsPqHXeq/Zs2aSoqk5rDq58zKyJnryJlr3JWvEydOuGU/AACgZrN80W632zVixAjt3btXn376qeMuuySFh4fr1KlTys7OdrrbnpWVpR49epS6z+nTp2vy5MmO5ZycHEVGRio+Pt5p/+WNNyUlRXFxcfLy8qrQvirD+CVbqjuEYjxthRpW/7BW/R6sfFP2NzYWj+taiVFZm9U/Z1ZEzlxHzlzj7nwVPQUGAAAubJYu2osK9u+++06pqakKCQlxWt+5c2d5eXkpJSVFI0aMkCQdPHhQO3fuVHJycqn79fHxkY+PT7F2Ly8vt12YunNf7uRKUVzV8k0tl+KzYn6rmlU/Z1ZGzlxHzlzjrnyRcwAAIFVz0X7s2DF9//33juW9e/cqIyNDwcHBioiI0LXXXqvt27fr/fffV0FBgeM99eDgYHl7eysoKEgTJkzQlClTFBISouDgYE2dOlVt27Z1zCYPAAAAAEBNVa1F+9atW9W7d2/HctEj62PHjlViYqLeffddSVKHDh2ctktNTVVsbKwk6dlnn5Wnp6dGjBih3Nxc9enTR0uWLJGHh0eVnAMAAAAAAJWlWov22NhYGWNKXX+2dUV8fX01b948zZs3z52hAQAAAABQ7az7gjMAAAAAABc4inYAAAAAACyKoh0AAAAAAIuiaAcAAAAAwKIo2gEAAAAAsCiKdgAAAAAALIqiHQAAAAAAi6JoBwAAAADAoijaAQAAAACwKIp2AAAAAAAsiqIdAAAAAACLomgHAAAAAMCiKNoBAAAAALAoinYAAAAAACyKoh0AAAAAAIuiaAcAAAAAwKIo2gEAAAAAsCiKdgAAAAAALIqiHQAAAAAAi6JoBwAAAADAoijaAQAAAACwKIp2AAAAAAAsiqIdAAAAAACLomgHAAAAAMCiKNoBAAAAALAoinYAAAAAACyKoh0AAAAAAIuiaAcAAAAAwKI8qzsAlN8Ni9KqOwQAAAAAQCXiTjsAAAAAABZF0Q4AAAAAgEVRtAMAAAAAYFEU7QAAAAAAWBRFOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFEU7AAAAAAAWRdEOAAAAAIBFUbQDAAAAAGBRFO0AAAAAAFgURTsAAAAAABZVrUX7Z599psGDBysiIkI2m02rV692Wm+MUWJioiIiIuTn56fY2Fjt2rXLqU9eXp7uuusuhYaGKiAgQEOGDNGBAweq8CwAAMDZ/PLLLxo1apRCQkLk7++vDh06aNu2bY71jPcAAJSuWov248ePq3379po/f36J65OTkzVnzhzNnz9f6enpCg8PV1xcnI4ePerok5CQoFWrVmnFihXauHGjjh07pkGDBqmgoKCqTgMAAJQiOztbPXv2lJeXlz788EN98803euaZZ1S3bl1HH8Z7AABK51mdB+/fv7/69+9f4jpjjObOnauHHnpIw4cPlyQtXbpUYWFhWr58uSZOnKgjR47o5Zdf1quvvqq+fftKkpYtW6bIyEh9/PHH6tevX5WdCwAAKG727NmKjIzU4sWLHW1NmjRx/DfjPQAAZ1etRfvZ7N27V5mZmYqPj3e0+fj4KCYmRps2bdLEiRO1bds22e12pz4RERGKjo7Wpk2bSh3E8/LylJeX51jOycmRJNntdtnt9grFXbR9RfdTFp62wko/RlUoOg9Xz6cqcmxVVfk5O1+QM9eRM9e4O1/nS97fffdd9evXT9ddd502bNigiy66SJMmTdKtt94qqfLG+6oY6xm3yo5/T1xHzlxHzlxHzlznzpyVdR/lKtr37t2rqKio8mxaZpmZmZKksLAwp/awsDDt37/f0cfb21v16tUr1qdo+5IkJSVp5syZxdrXrVsnf3//ioYuSUpJSXHLfs7mugaVfogqNaz+YZf6r1mzppIiqTmq4nN2viFnriNnrnFXvk6cOOGW/VS3H3/8UQsXLtTkyZP14IMPasuWLbr77rvl4+OjMWPGVNp4XxVjPeOW6/j3xHXkzHXkzHXkzHXuyFlZx/pyFe2XXnqpevXqpQkTJujaa6+Vr69veXZTJjabzWnZGFOs7Uzn6jN9+nRNnjzZsZyTk6PIyEjFx8erTp06FYrXbrcrJSVFcXFx8vLyqtC+zmX8ki2Vuv+q4mkr1LD6h7Xq92Dlm7JPs7B4XNdKjMraqvJzdr4gZ64jZ65xd76K7gzXdIWFherSpYtmzZolSerYsaN27dqlhQsXasyYMY5+7h7vq2KsZ9wqO/49cR05cx05cx05c507c1bWsb5cRfuOHTv0yiuvaMqUKbrzzjt1/fXXa8KECera1X2DUXh4uKTTf11v2LChoz0rK8vx1/jw8HCdOnVK2dnZTn99z8rKUo8ePUrdt4+Pj3x8fIq1e3l5ue3D6s59lcaVC4WaIN/Ucumc+Ielaj5n5xty5jpy5hp35et8yXnDhg3VunVrp7ZWrVrpnXfekVR5431VjPWMW67j3xPXkTPXkTPXkTPXuSNnZd2+XFVfdHS05syZo19++UWLFy9WZmamrrjiCrVp00Zz5szR77//Xp7dOomKilJ4eLjTYwenTp3Shg0bHAN0586d5eXl5dTn4MGD2rlz51mLdgAAUDV69uyp3bt3O7Xt2bNHjRs3lsR4DwDAuVToVq2np6eGDRumN998U7Nnz9YPP/ygqVOn6uKLL9aYMWN08ODBs25/7NgxZWRkKCMjQ9Lpd+UzMjL0008/yWazKSEhQbNmzdKqVau0c+dOjRs3Tv7+/ho5cqQkKSgoSBMmTNCUKVP0ySef6Msvv9SoUaPUtm1bx+yyAACg+tx7773avHmzZs2ape+//17Lly/XokWLdMcdd0gS4z0AAOdQodnjt27dqldeeUUrVqxQQECApk6dqgkTJujXX3/Vo48+qqFDh2rLltLfu966dat69+7tWC5692zs2LFasmSJpk2bptzcXE2aNEnZ2dnq1q2b1q1bp8DAQMc2zz77rDw9PTVixAjl5uaqT58+WrJkiTw8PCpyagAAwA0uu+wyrVq1StOnT9djjz2mqKgozZ07VzfddJOjD+M9AAClK1fRPmfOHC1evFi7d+/WgAED9O9//1sDBgxQrVqnb9xHRUXphRdeUMuWLc+6n9jYWBljSl1vs9mUmJioxMTEUvv4+vpq3rx5mjdvXnlOBQAAVLJBgwZp0KBBpa5nvAcAoHTlKtoXLlyom2++WePHj3dMIHOmRo0a6eWXX65QcAAAAAAAXMjKVbR/99135+zj7e2tsWPHlmf3AAAAAABA5ZyIbvHixXrrrbeKtb/11ltaunRphYMCAAAAAADlLNqffPJJhYaGFmtv0KCBZs2aVeGgAAAAAABAOYv2/fv3Kyoqqlh748aN9dNPP1U4KAAAAAAAUM6ivUGDBvrqq6+Kte/YsUMhISEVDgoAAAAAAJSzaL/hhht09913KzU1VQUFBSooKNCnn36qe+65RzfccIO7YwQAAAAA4IJUrtnjH3/8ce3fv199+vSRp+fpXRQWFmrMmDG80w4AAAAAgJuUq2j39vbWG2+8oX/84x/asWOH/Pz81LZtWzVu3Njd8QEAAAAAcMEqV9FepHnz5mrevLm7YgEAAAAAAH9RrqK9oKBAS5Ys0SeffKKsrCwVFhY6rf/000/dEhwAAAAAABeychXt99xzj5YsWaKBAwcqOjpaNpvN3XEBAAAAAHDBK1fRvmLFCr355psaMGCAu+MByuyGRWkub7Pitu6VEAkAAAAAVI5yfeWbt7e3Lr30UnfHAgAAAAAA/qJcRfuUKVP0z3/+U8YYd8cDAAAAAAD+v3I9Hr9x40alpqbqww8/VJs2beTl5eW0fuXKlW4JDgAAAACAC1m5iva6detq2LBh7o4FAAAAAAD8RbmK9sWLF7s7DgAAAAAAcIZyvdMuSfn5+fr444/1wgsv6OjRo5KkX3/9VceOHXNbcAAAAAAAXMjKdad9//79uvrqq/XTTz8pLy9PcXFxCgwMVHJysk6ePKnnn3/e3XECAAAAAHDBKded9nvuuUddunRRdna2/Pz8HO3Dhg3TJ5984rbgAAAAAAC4kJV79vj//ve/8vb2dmpv3LixfvnlF7cEBgAAAADAha5cd9oLCwtVUFBQrP3AgQMKDAyscFAAAAAAAKCcRXtcXJzmzp3rWLbZbDp27JhmzJihAQMGuCs2AAAAAAAuaOV6PP7ZZ59V79691bp1a508eVIjR47Ud999p9DQUL3++uvujhEAAAAAgAtSuYr2iIgIZWRk6PXXX9f27dtVWFioCRMm6KabbnKamA4AAAAAAJRfuYp2SfLz89PNN9+sm2++2Z3xAAAAAACA/69cRfu///3vs64fM2ZMuYIBAAAAAAD/p1xF+z333OO0bLfbdeLECXl7e8vf35+iHQAAAAAANyjX7PHZ2dlOP8eOHdPu3bt1xRVXMBEdAAAAAABuUq6ivSTNmjXTk08+WewuPAAAAAAAKB+3Fe2S5OHhoV9//dWduwQAAAAA4IJVrnfa3333XadlY4wOHjyo+fPnq2fPnm4JDAAAAACAC125ivZrrrnGadlms6l+/fq66qqr9Mwzz7gjLgAAAAAALnjlKtoLCwvdHQcAAAAAADhDuYp2uN8Ni9KqOwQAAAAAgMWUq2ifPHlymfvOmTOnPIcAAAAAAOCCV66i/csvv9T27duVn5+vFi1aSJL27NkjDw8PderUydHPZrO5J0oAAAAAAC5A5SraBw8erMDAQC1dulT16tWTJGVnZ2v8+PG68sorNWXKFLcGCQAAAADAhahc39P+zDPPKCkpyVGwS1K9evX0+OOPM3s8AAAAAABuUq6iPScnR7/99lux9qysLB09erTCQQEAAAAAgHIW7cOGDdP48eP19ttv68CBAzpw4IDefvttTZgwQcOHD3d3jAAAAAAAXJDKVbQ///zzGjhwoEaNGqXGjRurcePGuummm9S/f38tWLDAbcHl5+fr4YcfVlRUlPz8/NS0aVM99thjTt8Tb4xRYmKiIiIi5Ofnp9jYWO3atcttMQAAAAAAUF3KNRGdv7+/FixYoKeeeko//PCDjDG69NJLFRAQ4NbgZs+ereeff15Lly5VmzZttHXrVo0fP15BQUG65557JEnJycmaM2eOlixZoubNm+vxxx9XXFycdu/ercDAQLfGAwAAAABAVSrXnfYiBw8e1MGDB9W8eXMFBATIGOOuuCRJaWlpGjp0qAYOHKgmTZro2muvVXx8vLZu3Srp9F32uXPn6qGHHtLw4cMVHR2tpUuX6sSJE1q+fLlbYwEAAAAAoKqV6077oUOHNGLECKWmpspms+m7775T06ZNdcstt6hu3bpum0H+iiuu0PPPP689e/aoefPm2rFjhzZu3Ki5c+dKkvbu3avMzEzFx8c7tvHx8VFMTIw2bdqkiRMnlrjfvLw85eXlOZZzcnIkSXa7XXa7vUIxF23v6n48bYXn7nSeKjr3qshBRX+/VlHez9mFjJy5jpy5xt35Iu8AAEAqZ9F+7733ysvLSz/99JNatWrlaL/++ut17733uq1ov//++3XkyBG1bNlSHh4eKigo0BNPPKEbb7xRkpSZmSlJCgsLc9ouLCxM+/fvL3W/SUlJmjlzZrH2devWyd/f3y2xp6SkuNT/ugZuOWyNNqz+4Uo/xpo1ayr9GFXJ1c8ZyFl5kDPXuCtfJ06ccMt+AABAzVauon3dunX66KOPdPHFFzu1N2vW7KzFsqveeOMNLVu2TMuXL1ebNm2UkZGhhIQERUREaOzYsY5+NpvNaTtjTLG2v5o+fbomT57sWM7JyVFkZKTi4+NVp06dCsVst9uVkpKiuLg4eXl5lXm78Uu2VOi4NZmnrVDD6h/Wqt+DlW8q9MbGOS0e17VS919Vyvs5u5CRM9eRM9e4O19FT4EBAIALW7mK9uPHj5d4R/qPP/6Qj49PhYMqct999+mBBx7QDTfcIElq27at9u/fr6SkJI0dO1bh4eGSTt9xb9iwoWO7rKysYnff/8rHx6fEOL28vNx2Yerqviq7WK0J8k2tSs/D+VZ4uPMze6EgZ64jZ65xV77IOQAAkMo5EV2vXr3073//27Fss9lUWFiop556Sr1793ZbcCdOnFCtWs4henh4OL7yLSoqSuHh4U6PIp46dUobNmxQjx493BYHAAAAAADVoVx32p966inFxsZq69atOnXqlKZNm6Zdu3bp8OHD+u9//+u24AYPHqwnnnhCjRo1Ups2bfTll19qzpw5uvnmmyWd/mNBQkKCZs2apWbNmqlZs2aaNWuW/P39NXLkSLfFAQAAAABAdShX0d66dWt99dVXWrhwoTw8PHT8+HENHz5cd9xxh9Nj6hU1b948PfLII5o0aZKysrIUERGhiRMn6tFHH3X0mTZtmnJzczVp0iRlZ2erW7duWrduHd/RDgAAAACo8Vwu2u12u+Lj4/XCCy+UOAO7OwUGBmru3LmOr3gric1mU2JiohITEys1FgAAAAAAqprL77R7eXlp586dZ52dHQAAAAAAVFy5JqIbM2aMXn75ZXfHAgAAAAAA/qJc77SfOnVKL730klJSUtSlSxcFBAQ4rZ8zZ45bggMAAAAA4ELmUtH+448/qkmTJtq5c6c6deokSdqzZ49THx6bBwAAAADAPVwq2ps1a6aDBw8qNTVVknT99dfrX//6l8LCwiolOAAAAAAALmQuvdNujHFa/vDDD3X8+HG3BgQAAAAAAE4r10R0Rc4s4gEAAAAAgPu4VLTbbLZi76zzDjsAAAAAAJXDpXfajTEaN26cfHx8JEknT57U3//+92Kzx69cudJ9EQIAAAAAcIFyqWgfO3as0/KoUaPcGgwAAAAAAPg/LhXtixcvrqw4AAAAAADAGSo0ER0AAAAAAKg8FO0AAAAAAFgURTsAAAAAABZF0Q4AAAAAgEVRtAMAAAAAYFEU7QAAAAAAWBRFOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFEU7AAAAAAAW5VndAQBV6YZFaS71X3Fb90qKBAAAAADOjTvtAAAAAABYFEU7AACoMklJSbLZbEpISHC0GWOUmJioiIgI+fn5KTY2Vrt27XLaLi8vT3fddZdCQ0MVEBCgIUOG6MCBA1UcPQAAVY+iHQAAVIn09HQtWrRI7dq1c2pPTk7WnDlzNH/+fKWnpys8PFxxcXE6evSoo09CQoJWrVqlFStWaOPGjTp27JgGDRqkgoKCqj4NAACqFEU7AACodMeOHdNNN92kF198UfXq1XO0G2M0d+5cPfTQQxo+fLiio6O1dOlSnThxQsuXL5ckHTlyRC+//LKeeeYZ9e3bVx07dtSyZcv09ddf6+OPP66uUwIAoEowER0AAKh0d9xxhwYOHKi+ffvq8ccfd7Tv3btXmZmZio+Pd7T5+PgoJiZGmzZt0sSJE7Vt2zbZ7XanPhEREYqOjtamTZvUr1+/YsfLy8tTXl6eYzknJ0eSZLfbZbfbK3QuRdt72grLtd2FqOjcL+QcuIqcuY6cuY6cuc6dOSvrPijaAQBApVqxYoW2b9+u9PT0YusyMzMlSWFhYU7tYWFh2r9/v6OPt7e30x36oj5F258pKSlJM2fOLNa+bt06+fv7l+s8zjSs/mGX+q9Zs8Ytx63JUlJSqjuEGoecuY6cuY6cuc4dOTtx4kSZ+lG0AwCASvPzzz/rnnvu0bp16+Tr61tqP5vN5rRsjCnWdqaz9Zk+fbomT57sWM7JyVFkZKTi4+NVp04dF86gOLvdrpSUFK36PVj5puxvGi4e17VCx63JinIWFxcnLy+v6g6nRiBnriNnriNnrnNnzoqeAjsXinYAAFBptm3bpqysLHXu3NnRVlBQoM8++0zz58/X7t27JZ2+m96wYUNHn6ysLMfd9/DwcJ06dUrZ2dlOd9uzsrLUo0ePEo/r4+MjHx+fYu1eXl5uuzDNN7VcKtq5IHZv/i8U5Mx15Mx15Mx17shZWbdnIjoAAFBp+vTpo6+//loZGRmOny5duuimm25SRkaGmjZtqvDwcKfHDE+dOqUNGzY4CvLOnTvLy8vLqc/Bgwe1c+fOUot2AADOF9xpBwAAlSYwMFDR0dFObQEBAQoJCXG0JyQkaNasWWrWrJmaNWumWbNmyd/fXyNHjpQkBQUFacKECZoyZYpCQkIUHBysqVOnqm3bturbt2+VnxMAAFWJoh0AAFSradOmKTc3V5MmTVJ2dra6deumdevWKTAw0NHn2Weflaenp0aMGKHc3Fz16dNHS5YskYeHRzVG7pobFqW5vM2K27pXQiQAgJqEoh0AAFSp9evXOy3bbDYlJiYqMTGx1G18fX01b948zZs3r3KDAwDAYninHQAAAAAAi6JoBwAAAADAoijaAQAAAACwKIp2AAAAAAAsiqIdAAAAAACLomgHAAAAAMCiLF+0//LLLxo1apRCQkLk7++vDh06aNu2bY71xhglJiYqIiJCfn5+io2N1a5du6oxYgAAAAAA3MPSRXt2drZ69uwpLy8vffjhh/rmm2/0zDPPqG7duo4+ycnJmjNnjubPn6/09HSFh4crLi5OR48erb7AAQAAAABwA8/qDuBsZs+ercjISC1evNjR1qRJE8d/G2M0d+5cPfTQQxo+fLgkaenSpQoLC9Py5cs1ceLEqg4ZAAAAAAC3sfSd9nfffVddunTRddddpwYNGqhjx4568cUXHev37t2rzMxMxcfHO9p8fHwUExOjTZs2VUfIAAAAAAC4jaXvtP/4449auHChJk+erAcffFBbtmzR3XffLR8fH40ZM0aZmZmSpLCwMKftwsLCtH///lL3m5eXp7y8PMdyTk6OJMlut8tut1co5qLtXd2Pp62wQsetyYrO3Yo5qOjnobKU93N2ISNnriNnrnF3vsg7AACQLF60FxYWqkuXLpo1a5YkqWPHjtq1a5cWLlyoMWPGOPrZbDan7Ywxxdr+KikpSTNnzizWvm7dOvn7+7sl9pSUFJf6X9fALYet0YbVP1zdIRSzZs2a6g7hrFz9nIGclQc5c4278nXixAm37AcAANRsli7aGzZsqNatWzu1tWrVSu+8844kKTw8XJKUmZmphg0bOvpkZWUVu/v+V9OnT9fkyZMdyzk5OYqMjFR8fLzq1KlToZjtdrtSUlIUFxcnLy+vMm83fsmWCh23JvO0FWpY/cNa9Xuw8o213thYPK5rdYdQovJ+zi5k5Mx15Mw17s5X0VNgAADgwmbpor1nz57avXu3U9uePXvUuHFjSVJUVJTCw8OVkpKijh07SpJOnTqlDRs2aPbs2aXu18fHRz4+PsXavby83HZh6uq+rFasVod8U8tyebB6oeLOz+yFgpy5jpy5xl35IucAAECyeNF+7733qkePHpo1a5ZGjBihLVu2aNGiRVq0aJGk04/FJyQkaNasWWrWrJmaNWumWbNmyd/fXyNHjqzm6AEAAAAAqBhLF+2XXXaZVq1apenTp+uxxx5TVFSU5s6dq5tuusnRZ9q0acrNzdWkSZOUnZ2tbt26ad26dQoMDKzGyAEAAAAAqDhLF+2SNGjQIA0aNKjU9TabTYmJiUpMTKy6oAAAAAAAqALWeoEYAAAAAAA4ULQDAAAAAGBRFO0AAAAAAFgURTsAAAAAABZF0Q4AAAAAgEVRtAMAAAAAYFEU7QAAAAAAWJTlv6cdqGluWJTm8jYrbuteCZEAAAAAqOm40w4AAAAAgEVRtAMAAAAAYFEU7QAAAAAAWBRFOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFEU7AAAAAAAWRdEOAAAAAIBFUbQDAAAAAGBRFO0AAAAAAFgURTsAAAAAABblWd0BAFZ2w6K06g4BAAAAwAWMO+0AAAAAAFgURTsAAAAAABZF0Q4AAAAAgEVRtAMAAAAAYFEU7QAAAAAAWBRFOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFEU7AAAAAAAWRdEOAAAAAIBFUbQDAAAAAGBRntUdwPlq/JItyjf8TQQAAAAAUH5UlQAAAAAAWBRFOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFEU7AAAAAAAWRdEOAAAAAIBF8T3tgAXcsCjNpf6etkJd16CSggEAAABgGTXqTntSUpJsNpsSEhIcbcYYJSYmKiIiQn5+foqNjdWuXbuqL0gAAAAAANykxhTt6enpWrRokdq1a+fUnpycrDlz5mj+/PlKT09XeHi44uLidPTo0WqKFAAAAAAA96gRRfuxY8d000036cUXX1S9evUc7cYYzZ07Vw899JCGDx+u6OhoLV26VCdOnNDy5curMWIAAAAAACquRrzTfscdd2jgwIHq27evHn/8cUf73r17lZmZqfj4eEebj4+PYmJitGnTJk2cOLHE/eXl5SkvL8+xnJOTI0my2+2y2+0VirVoe09bYYX2cyEpyhU5K7uiXFX083ohKcoVOSs7cuYad+eLvAMAAKkGFO0rVqzQ9u3blZ6eXmxdZmamJCksLMypPSwsTPv37y91n0lJSZo5c2ax9nXr1snf37+CEZ82rP5ht+znQkLOXJeSklLdIdQ45Mx15Mw17srXiRMn3LIfAABQs1m6aP/55591zz33aN26dfL19S21n81mc1o2xhRr+6vp06dr8uTJjuWcnBxFRkYqPj5ederUqVDMdrtdKSkpWvV7sPJNjXj7oNp52go1rP5hcuaCopzFxcXJy8urusOpEYr+3yRnZUfOXOPufBU9BQYAAC5sli7at23bpqysLHXu3NnRVlBQoM8++0zz58/X7t27JZ2+496wYUNHn6ysrGJ33//Kx8dHPj4+xdq9vLzcdmGab2pRgLqInLnOnZ/ZCwU5cx05c4278kXOAQCAZPGJ6Pr06aOvv/5aGRkZjp8uXbropptuUkZGhpo2barw8HCnRxFPnTqlDRs2qEePHtUYOQAAAAAAFWfpO+2BgYGKjo52agsICFBISIijPSEhQbNmzVKzZs3UrFkzzZo1S/7+/ho5cmR1hAwAAAAAgNtYumgvi2nTpik3N1eTJk1Sdna2unXrpnXr1ikwMLC6QwMAAAAAoEJqXNG+fv16p2WbzabExEQlJiZWSzwAAAAAAFQWS7/TDgAAAADAhYyiHQAAAAAAi6JoBwAAAADAoijaAQAAAACwKIp2AAAAAAAsiqIdAAAAAACLomgHAAAAAMCiKNoBAEClSUpK0mWXXabAwEA1aNBA11xzjXbv3u3UxxijxMRERUREyM/PT7Gxsdq1a5dTn7y8PN11110KDQ1VQECAhgwZogMHDlTlqQAAUC0o2gEAQKXZsGGD7rjjDm3evFkpKSnKz89XfHy8jh8/7uiTnJysOXPmaP78+UpPT1d4eLji4uJ09OhRR5+EhAStWrVKK1as0MaNG3Xs2DENGjRIBQUF1XFaAABUGc/qDgBA1blhUZpL/Vfc1r2SIgFwoVi7dq3T8uLFi9WgQQNt27ZNvXr1kjFGc+fO1UMPPaThw4dLkpYuXaqwsDAtX75cEydO1JEjR/Tyyy/r1VdfVd++fSVJy5YtU2RkpD7++GP169evys8LAICqQtEOAACqzJEjRyRJwcHBkqS9e/cqMzNT8fHxjj4+Pj6KiYnRpk2bNHHiRG3btk12u92pT0REhKKjo7Vp06YSi/a8vDzl5eU5lnNyciRJdrtddru9QudQtL2nrbBC+3HlWDVd0XmcL+dTFciZ68iZ68iZ69yZs7Lug6IdAABUCWOMJk+erCuuuELR0dGSpMzMTElSWFiYU9+wsDDt37/f0cfb21v16tUr1qdo+zMlJSVp5syZxdrXrVsnf3//Cp+LJA2rf9gt+zmbNWvWVPoxqlJKSkp1h1DjkDPXkTPXkTPXuSNnJ06cKFM/inYAAFAl7rzzTn311VfauHFjsXU2m81p2RhTrO1MZ+szffp0TZ482bGck5OjyMhIxcfHq06dOuWI/v/Y7XalpKRo1e/ByjeVOz3Q4nFdK3X/VaUoZ3FxcfLy8qrucGoEcuY6cuY6cuY6d+as6Cmwc6FoBwAAle6uu+7Su+++q88++0wXX3yxoz08PFzS6bvpDRs2dLRnZWU57r6Hh4fr1KlTys7OdrrbnpWVpR49epR4PB8fH/n4+BRr9/LyctuFab6pVelF+/l2Ee3O/F8oyJnryJnryJnr3JGzsm5P0Q6gVK5OXCcxeR0AZ8YY3XXXXVq1apXWr1+vqKgop/VRUVEKDw9XSkqKOnbsKEk6deqUNmzYoNmzZ0uSOnfuLC8vL6WkpGjEiBGSpIMHD2rnzp1KTk6u2hMCAKCKUbQDAIBKc8cdd2j58uX6z3/+o8DAQMc76EFBQfLz85PNZlNCQoJmzZqlZs2aqVmzZpo1a5b8/f01cuRIR98JEyZoypQpCgkJUXBwsKZOnaq2bds6ZpMHAOB8RdEOAAAqzcKFCyVJsbGxTu2LFy/WuHHjJEnTpk1Tbm6uJk2apOzsbHXr1k3r1q1TYGCgo/+zzz4rT09PjRgxQrm5uerTp4+WLFkiDw+PqjoVAACqBUU7AACoNMaYc/ax2WxKTExUYmJiqX18fX01b948zZs3z43RAQBgfZU7ewoAAAAAACg3inYAAAAAACyKoh0AAAAAAIuiaAcAAAAAwKKYiA6owcYv2aJ8w9/eAAAAgPMVV/sAAAAAAFgURTsAAAAAABZF0Q4AAAAAgEVRtAMAAAAAYFEU7QAAAAAAWBRFOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFEU7AAAAAAAW5VndAQDADYvSXN5mxW3dKyESAAAAwFq40w4AAAAAgEVRtAMAAAAAYFEU7QAAAAAAWBRFOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFN/TDqBGcvW73T1thbquQSUFAwAAAFQSS99pT0pK0mWXXabAwEA1aNBA11xzjXbv3u3UxxijxMRERUREyM/PT7Gxsdq1a1c1RQwAAAAAgPtYumjfsGGD7rjjDm3evFkpKSnKz89XfHy8jh8/7uiTnJysOXPmaP78+UpPT1d4eLji4uJ09OjRaowcAAAAAICKs/Tj8WvXrnVaXrx4sRo0aKBt27apV69eMsZo7ty5euihhzR8+HBJ0tKlSxUWFqbly5dr4sSJ1RE2AAAAAABuYemi/UxHjhyRJAUHB0uS9u7dq8zMTMXHxzv6+Pj4KCYmRps2bSq1aM/Ly1NeXp5jOScnR5Jkt9tlt9srFGPR9p62wgrt50JSlCtyVnZWzll5/h+qivMoOkZF/x+/kBTlipyVjbvzRd4BAIBUg4p2Y4wmT56sK664QtHR0ZKkzMxMSVJYWJhT37CwMO3fv7/UfSUlJWnmzJnF2tetWyd/f3+3xDus/mG37OdCQs5cZ8WcrVmzxuVtqnKCuJSUlKo72HmCnLnGXfk6ceKEW/YDAABqthpTtN9555366quvtHHjxmLrbDab07IxpljbX02fPl2TJ092LOfk5CgyMlLx8fGqU6dOheK02+1KSUnRqt+DlW8sPWWAZXjaCjWs/mFy5gIr52zxuK4ubzN+yZZKiMRZUc7i4uLk5eVVaccpz7mUJ2dVoejfs8rO2fnC3fkqegoMAABc2GpE0X7XXXfp3Xff1WeffaaLL77Y0R4eHi7p9B33hg0bOtqzsrKK3X3/Kx8fH/n4+BRr9/LyctuFab6pZbliyurImeusmLPy/D9Ulefgzv/PS1Kec7F6QVzZOTvfuCtf5BwAAEgWL9qNMbrrrru0atUqrV+/XlFRUU7ro6KiFB4erpSUFHXs2FGSdOrUKW3YsEGzZ8+ujpCBC56r358OAAAAoHSWLtrvuOMOLV++XP/5z38UGBjoeIc9KChIfn5+stlsSkhI0KxZs9SsWTM1a9ZMs2bNkr+/v0aOHFnN0QMAAAAAUDGWLtoXLlwoSYqNjXVqX7x4scaNGydJmjZtmnJzczVp0iRlZ2erW7duWrdunQIDA6s4WgDnI54cAAAAQHWydNFujDlnH5vNpsTERCUmJlZ+QAAAAAAAVCFrzWAFAAAAAAAcLH2nHQAuBOV5BH/Fbd0rIRIAAABYDXfaAQAAAACwKIp2AAAAAAAsisfjAeAC4epj+J62Ql3XoJKCAQAAQJlwpx0AAAAAAIviTjsA4KzGL9mifFO2v/EyQR4AAIB7cacdAAAAAACLomgHAAAAAMCiKNoBAAAAALAoinYAAAAAACyKiegAAAAsytWvapSYEBIAzjfcaQcAAAAAwKIo2gEAAAAAsCgejwdwQXHlO8cBAACA6saVKwAAAAAAFkXRDgAAAACARfF4PADUQOWZURoAAAA1D3faAQAAAACwKIp2AAAAAAAsiqIdAAAAAACLomgHAAAAAMCiKNoBAAAAALAoinYAAAAAACyKoh0AAAAAAIuiaAcAAAAAwKIo2gEAAAAAsCjP6g4AAHBhu2FRmsvbrLitu2WPAwAA4E7caQcAAAAAwKIo2gEAAAAAsCiKdgAAAAAALIqiHQAAAAAAi6JoBwAAAADAoijaAQAAAACwKIp2AAAAAAAsiu9pBwC4TXm+Cx0AAACl4047AAAAAAAWRdEOAAAAAIBF8Xg8AKDG4TF8oHTl+f9jxW3dKyESAIA7cKcdAAAAAACL4k47AAClcOWOpaetUNc1qMRgAADABYk77QAAAAAAWNR5U7QvWLBAUVFR8vX1VefOnfX5559Xd0gAAMCNGOsBABei86Jof+ONN5SQkKCHHnpIX375pa688kr1799fP/30U3WHBgAA3ICxHgBwoTovivY5c+ZowoQJuuWWW9SqVSvNnTtXkZGRWrhwYXWHBgAA3ICxHgBwoarxE9GdOnVK27Zt0wMPPODUHh8fr02bNpW4TV5envLy8hzLR44ckSQdPnxYdru9QvHY7XadOHFChSePypjz4m8ila7QVkjOXETOXEfOXEfOXFOUr0OHDsnLy6vC+zt69KgkyRhT4X3VdIz1le/6f62r1P172Ao1OPSExj7/iQosmLMFN3Wu7hCKKfqcuevflAsBOXMdOXOdO3NW1rG+xhftf/zxhwoKChQWFubUHhYWpszMzBK3SUpK0syZM4u1R0VFVUqMOLfXqzuAGoicuY6cuY6cuaYy8nX06FEFBQVVwp5rDsb684OV/z15857qjgDAhexcY32NL9qL2Gw2p2VjTLG2ItOnT9fkyZMdy4WFhTp8+LBCQkJK3aascnJyFBkZqZ9//ll16tSp0L4uFOTMdeTMdeTMdeTMNe7OlzFGR48eVUREhBuiOz8w1tdc5Mx15Mx15Mx15Mx17sxZWcf6Gl+0h4aGysPDo9hf2rOysor9Rb6Ij4+PfHx8nNrq1q3r1rjq1KnDB99F5Mx15Mx15Mx15Mw17szXhX6HvQhj/fmDnLmOnLmOnLmOnLnOXTkry1hvvZeKXOTt7a3OnTsrJSXFqT0lJUU9evSopqgAAIC7MNYDAC5kNf5OuyRNnjxZo0ePVpcuXdS9e3ctWrRIP/30k/7+979Xd2gAAMANGOsBABeq86Jov/7663Xo0CE99thjOnjwoKKjo7VmzRo1bty4ymPx8fHRjBkzij2Sh9KRM9eRM9eRM9eRM9eQr8rFWF+zkTPXkTPXkTPXkTPXVUfObIbvkgEAAAAAwJJq/DvtAAAAAACcryjaAQAAAACwKIp2AAAAAAAsiqIdAAAAAACLomh3swULFigqKkq+vr7q3LmzPv/88+oOqVokJSXpsssuU2BgoBo0aKBrrrlGu3fvdupjjFFiYqIiIiLk5+en2NhY7dq1y6lPXl6e7rrrLoWGhiogIEBDhgzRgQMHqvJUqkVSUpJsNpsSEhIcbeSruF9++UWjRo1SSEiI/P391aFDB23bts2xnpw5y8/P18MPP6yoqCj5+fmpadOmeuyxx1RYWOjoc6Hn7LPPPtPgwYMVEREhm82m1atXO613V36ys7M1evRoBQUFKSgoSKNHj9aff/5ZyWcHd2GsP42xvmIY68uO8d41jPfnVuPGewO3WbFihfHy8jIvvvii+eabb8w999xjAgICzP79+6s7tCrXr18/s3jxYrNz506TkZFhBg4caBo1amSOHTvm6PPkk0+awMBA884775ivv/7aXH/99aZhw4YmJyfH0efvf/+7ueiii0xKSorZvn276d27t2nfvr3Jz8+vjtOqElu2bDFNmjQx7dq1M/fcc4+jnXw5O3z4sGncuLEZN26c+eKLL8zevXvNxx9/bL7//ntHH3Lm7PHHHzchISHm/fffN3v37jVvvfWWqV27tpk7d66jz4WeszVr1piHHnrIvPPOO0aSWbVqldN6d+Xn6quvNtHR0WbTpk1m06ZNJjo62gwaNKiqThMVwFj/fxjry4+xvuwY713HeH9uNW28p2h3o65du5q///3vTm0tW7Y0DzzwQDVFZB1ZWVlGktmwYYMxxpjCwkITHh5unnzySUefkydPmqCgIPP8888bY4z5888/jZeXl1mxYoWjzy+//GJq1apl1q5dW7UnUEWOHj1qmjVrZlJSUkxMTIxjICdfxd1///3miiuuKHU9OStu4MCB5uabb3ZqGz58uBk1apQxhpyd6cxB3F35+eabb4wks3nzZkeftLQ0I8n873//q+SzQkUx1peOsb5sGOtdw3jvOsZ719SE8Z7H493k1KlT2rZtm+Lj453a4+PjtWnTpmqKyjqOHDkiSQoODpYk7d27V5mZmU758vHxUUxMjCNf27Ztk91ud+oTERGh6Ojo8zand9xxhwYOHKi+ffs6tZOv4t5991116dJF1113nRo0aKCOHTvqxRdfdKwnZ8VdccUV+uSTT7Rnzx5J0o4dO7Rx40YNGDBAEjk7F3flJy0tTUFBQerWrZujz+WXX66goKDzPoc1HWP92THWlw1jvWsY713HeF8xVhzvPStyQvg/f/zxhwoKChQWFubUHhYWpszMzGqKyhqMMZo8ebKuuOIKRUdHS5IjJyXla//+/Y4+3t7eqlevXrE+52NOV6xYoe3btys9Pb3YOvJV3I8//qiFCxdq8uTJevDBB7Vlyxbdfffd8vHx0ZgxY8hZCe6//34dOXJELVu2lIeHhwoKCvTEE0/oxhtvlMTn7FzclZ/MzEw1aNCg2P4bNGhw3uewpmOsLx1jfdkw1ruO8d51jPcVY8XxnqLdzWw2m9OyMaZY24Xmzjvv1FdffaWNGzcWW1eefJ2POf355591zz33aN26dfL19S21H/n6P4WFherSpYtmzZolSerYsaN27dqlhQsXasyYMY5+5Oz/vPHGG1q2bJmWL1+uNm3aKCMjQwkJCYqIiNDYsWMd/cjZ2bkjPyX1v5ByWNMx1hfHWH9ujPXlw3jvOsZ797DSeM/j8W4SGhoqDw+PYn81ycrKKvZXmgvJXXfdpXfffVepqam6+OKLHe3h4eGSdNZ8hYeH69SpU8rOzi61z/li27ZtysrKUufOneXp6SlPT09t2LBB//rXv+Tp6ek4X/L1fxo2bKjWrVs7tbVq1Uo//fSTJD5jJbnvvvv0wAMP6IYbblDbtm01evRo3XvvvUpKSpJEzs7FXfkJDw/Xb7/9Vmz/v//++3mfw5qOsb5kjPVlw1hfPoz3rmO8rxgrjvcU7W7i7e2tzp07KyUlxak9JSVFPXr0qKaoqo8xRnfeeadWrlypTz/9VFFRUU7ro6KiFB4e7pSvU6dOacOGDY58de7cWV5eXk59Dh48qJ07d553Oe3Tp4++/vprZWRkOH66dOmim266SRkZGWratCn5OkPPnj2LfbXQnj171LhxY0l8xkpy4sQJ1arl/M++h4eH4ytgyNnZuSs/3bt315EjR7RlyxZHny+++EJHjhw573NY0zHWO2Osdw1jffkw3ruO8b5iLDneuzRtHc6q6GtgXn75ZfPNN9+YhIQEExAQYPbt21fdoVW522+/3QQFBZn169ebgwcPOn5OnDjh6PPkk0+aoKAgs3LlSvP111+bG2+8scSvUrj44ovNxx9/bLZv326uuuqq8+arJs7lrzPKGkO+zrRlyxbj6elpnnjiCfPdd9+Z1157zfj7+5tly5Y5+pAzZ2PHjjUXXXSR4ytgVq5caUJDQ820adMcfS70nB09etR8+eWX5ssvvzSSzJw5c8yXX37p+Dovd+Xn6quvNu3atTNpaWkmLS3NtG3blq98qyEY6/8PY33FMdafG+O96xjvz62mjfcU7W723HPPmcaNGxtvb2/TqVMnx9eeXGgklfizePFiR5/CwkIzY8YMEx4ebnx8fEyvXr3M119/7bSf3Nxcc+edd5rg4GDj5+dnBg0aZH766acqPpvqceZATr6Ke++990x0dLTx8fExLVu2NIsWLXJaT86c5eTkmHvuucc0atTI+Pr6mqZNm5qHHnrI5OXlOfpc6DlLTU0t8d+usWPHGmPcl59Dhw6Zm266yQQGBprAwEBz0003mezs7Co6S1QUY/1pjPUVx1hfNoz3rmG8P7eaNt7bjDHGtXvzAAAAAACgKvBOOwAAAAAAFkXRDgAAAACARVG0AwAAAABgURTtAAAAAABYFEU7AAAAAAAWRdEOAAAAAIBFUbQDAAAAAGBRFO0AXNakSRPNnTu3usMAAACVhLEesA6KdqCGef755xUYGKj8/HxH27Fjx+Tl5aUrr7zSqe/nn38um82mPXv2VHWYysnJ0UMPPaSWLVvK19dX4eHh6tu3r1auXCljTJXGwoUHAKAmYax3HWM9zmee1R0AANf07t1bx44d09atW3X55ZdLOj1gh4eHKz09XSdOnJC/v78kaf369YqIiFDz5s1dPk5BQYFsNptq1XL9b3t//vmnrrjiCh05ckSPP/64LrvsMnl6emrDhg2aNm2arrrqKtWtW9fl/QIAcCFgrAfwV9xpB2qYFi1aKCIiQuvXr3e0rV+/XkOHDtUll1yiTZs2ObX37t1bkpSdna0xY8aoXr168vf3V//+/fXdd985+i5ZskR169bV+++/r9atW8vHx0f79+9XVlaWBg8eLD8/P0VFRem11147Z4wPPvig9u3bpy+++EJjx45V69at1bx5c916663KyMhQ7dq1yxRTYmKiOnTo4LTvuXPnqkmTJo7lcePG6ZprrtHTTz+thg0bKiQkRHfccYfsdrskKTY2Vvv379e9994rm80mm81W5lwDAFAdGOsZ64G/omgHaqDY2FilpqY6llNTUxUbG6uYmBhH+6lTp5SWluYYyMeNG6etW7fq3XffVVpamowxGjBggGPAk6QTJ04oKSlJL730knbt2qUGDRpo3Lhx2rdvnz799FO9/fbbWrBggbKyskqNrbCwUCtWrNBNN92kiIiIYutr164tT0/PMsdUFqmpqfrhhx+UmpqqpUuXasmSJVqyZIkkaeXKlbr44ov12GOP6eDBgzp48KBL+wYAoDow1jtjrMeFjMfjgRooNjZW9957r/Lz85Wbm6svv/xSvXr1UkFBgf71r39JkjZv3qzc3Fz17t1b3333nd59913997//VY8ePSRJr732miIjI7V69Wpdd911kiS73a4FCxaoffv2kqQ9e/boww8/1ObNm9WtWzdJ0ssvv6xWrVqVGtsff/yh7OxstWzZ8qznUNaYyqJevXqaP3++PDw81LJlSw0cOFCffPKJbr31VgUHB8vDw0OBgYEKDw8v8z4BAKhOjPXOGOtxIeNOO1AD9e7dW8ePH1d6ero+//xzNW/eXA0aNFBMTIzS09N1/PhxrV+/Xo0aNVLTpk317bffytPT0zEYS1JISIhatGihb7/91tHm7e2tdu3aOZaLtuvSpYujrWXLlmd9R61o4plzPZpW1pjKok2bNvLw8HAsN2zY8Kx3CAAAsDrGemeM9biQcacdqIEuvfRSXXzxxUpNTVV2drZiYmIkSeHh4YqKitJ///tfpaam6qqrrpKkUmdwNcY4Dbh+fn5Oy2UdlP+qfv36qlev3jkH47LEVKtWrWL9SnqczsvLy2nZZrOpsLCwzDEDAGA1jPXOGOtxIeNOO1BD9e7dW+vXr9f69esVGxvraI+JidFHH32kzZs3O95xa926tfLz8/XFF184+h06dEh79uw56+NvrVq1Un5+vrZu3epo2717t/78889St6lVq5auv/56vfbaa/r111+LrT9+/Ljy8/PLFFP9+vWVmZnpNJhnZGSUeuzSeHt7q6CgwOXtAACoToz1ZcdYj/MZRTtQQ/Xu3VsbN25URkaG46/v0umB/MUXX9TJkycdA3mzZs00dOhQ3Xrrrdq4caN27NihUaNG6aKLLtLQoUNLPUaLFi109dVX69Zbb9UXX3yhbdu26ZZbbpGfn99ZY5s1a5YiIyPVrVs3/fvf/9Y333yj7777Tq+88oo6dOigY8eOlSmm2NhY/f7770pOTtYPP/yg5557Th9++KHLuWrSpIk+++wz/fLLL/rjjz9c3h4AgOrAWF92jPU4n1G0AzVU7969lZubq0svvVRhYWGO9piYGB09elSXXHKJIiMjHe2LFy9W586dNWjQIHXv3l3GGK1Zs6bY42ZnWrx4sSIjIxUTE6Phw4frtttuU4MGDc66Tb169bR582aNGjVKjz/+uDp27Kgrr7xSr7/+up566ikFBQWVKaZWrVppwYIFeu6559S+fXtt2bJFU6dOdTlXjz32mPbt26dLLrlE9evXd3l7AACqA2N92THW43xmM6W9bAIAAID/154d0wAAADAI8+96MsbRuiAAwJXTDgAAAFGiHQAAAKJEOwAAAESJdgAAAIgS7QAAABAl2gEAACBKtAMAAECUaAcAAIAo0Q4AAABRoh0AAACiRDsAAABEiXYAAACIGpVHX1MAb8AmAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming your DataFrame 'df' and list calculations are correct\n",
        "text_word_count = [len(i.split()) for i in df['text']]\n",
        "headline_word_count = [len(j.split()) for j in df['headline']]\n",
        "length_df = pd.DataFrame({'text': text_word_count, 'headline': headline_word_count})\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
        "length_df['text'].hist(bins=30, range=[0, 1000], alpha=0.75, ax=axes[0])\n",
        "length_df['headline'].hist(bins=30, range=[0, 1000], alpha=0.75, ax=axes[1])\n",
        "\n",
        "axes[0].set_title('Text Word Count')\n",
        "axes[1].set_title('Headline Word Count')\n",
        "axes[0].set_xlabel('Word Count')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[1].set_xlabel('Word Count')\n",
        "\n",
        "plt.suptitle('Word Count Distribution in Text and Headlines')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9ApVnUxJPXYT"
      },
      "outputs": [],
      "source": [
        "# From the graph\n",
        "# We can fix maximum length of text = 150 since most of the reviews have a length of 150 and maximum headline length of 50, since maximum headlines are of size 40\n",
        "\n",
        "max_len_text= 150\n",
        "max_len_headline=50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBPwN7lmGj6v",
        "outputId": "e0681665-1529-4429-91e8-9ff5b267e57f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9979892761394102\n"
          ]
        }
      ],
      "source": [
        "cnt=0\n",
        "for i in df['text']:\n",
        "    if(len(i.split())<=1500):\n",
        "        cnt=cnt+1\n",
        "print(cnt/len(df['text']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbjBRO7BIobl"
      },
      "source": [
        "Selecting text and headlines below the maximum lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DLEMfHWTIoHy"
      },
      "outputs": [],
      "source": [
        "text1 =np.array(df['text'])\n",
        "headline1=np.array(df['headline'])\n",
        "\n",
        "short_text=[]\n",
        "short_summary=[]\n",
        "\n",
        "for i in range(len(text1)):\n",
        "    if(len(headline1[i].split())<=50 and len(text1[i].split())<=150):\n",
        "        short_text.append(text1[i])\n",
        "        short_summary.append(headline1[i])\n",
        "\n",
        "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-2cdR4iXImwQ"
      },
      "outputs": [],
      "source": [
        "# Validating the lengths\n",
        "\n",
        "text1 =np.array(df['text'])\n",
        "headline1=np.array(df['summary'])\n",
        "\n",
        "\n",
        "for i in range(len(text1)):\n",
        "    if(len(headline1[i].split())>=150):\n",
        "      print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytlW3ABFo2-M",
        "outputId": "c3272c34-4e3b-4cce-93ab-d98552187991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "guy like every facebook status update instagram picture post better yet frequently comment posts sign wants interact may indicate likes look see comments people posts well comments frequently may reveal avid social media user however rarely likes comments posts sign might feelings publicly comments one photos posts respond eager start conversation way may reveal likes least enjoys messaging example may say great photo could respond saying vancouver last week beautiful city guy met starts liking commenting old photos sig interested means spent time looking back old photos probably wants know better enjoys looking pictures guy likes want connect follow variety different social media platforms example may add facebook snapchat start following twitter instagram adding number social media sites likely sign wants look posts photos selfies attempt get know better\n",
            "see interacts posts reply comments notice comments old pictures posts check see added multiple social media platforms\n"
          ]
        }
      ],
      "source": [
        "print(df['text'][50],df['summary'][50],sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGoWWbOiXbxK"
      },
      "source": [
        "**Splitting data into train, test  --  70 - 30**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "R8BhLDriRM_m"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_texts = df['text'].astype(str).tolist()\n",
        "y_texts = ['starttoken ' + str(summary) + ' endtoken' for summary in df['summary'].astype(str).tolist()]\n",
        "\n",
        "# Split raw text and summaries\n",
        "x_tr_texts, x_val_texts, y_tr_texts, y_val_texts = train_test_split(\n",
        "    df['text'].astype(str), df['summary'].astype(str),\n",
        "    test_size=0.3, random_state=0, shuffle=True\n",
        ")\n",
        "\n",
        "# Add start and end tokens to summaries\n",
        "y_tr_texts = ['starttoken ' + text + ' endtoken' for text in y_tr_texts]\n",
        "y_val_texts = ['starttoken ' + text + ' endtoken' for text in y_val_texts]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['text', 'summary'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qelmpQLQD5F",
        "outputId": "5bbbcc53-3322-46d1-a221-201e5169d8d2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# 1. Tokenizer for input text (x)\n",
        "x_tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "x_tokenizer.fit_on_texts(x_tr_texts)\n",
        "\n",
        "# 2. Tokenizer for summaries (y)\n",
        "y_tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "y_tokenizer.fit_on_texts(y_tr_texts)\n",
        "\n",
        "# 3. Convert texts to sequences\n",
        "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr_texts)\n",
        "x_val_seq = x_tokenizer.texts_to_sequences(x_val_texts)\n",
        "\n",
        "y_tr_seq = y_tokenizer.texts_to_sequences(y_tr_texts)\n",
        "y_val_seq = y_tokenizer.texts_to_sequences(y_val_texts)\n",
        "\n",
        "# 4. Define max lengths\n",
        "max_len_text = 48\n",
        "max_len_summary = 49\n",
        "\n",
        "# 5. Pad sequences\n",
        "x_tr = pad_sequences(x_tr_seq, maxlen=max_len_text, padding='post')\n",
        "x_val = pad_sequences(x_val_seq, maxlen=max_len_text, padding='post')\n",
        "\n",
        "y_tr = pad_sequences(y_tr_seq, maxlen=max_len_summary, padding='post')\n",
        "y_val = pad_sequences(y_val_seq, maxlen=max_len_summary, padding='post')\n",
        "\n",
        "# 6. Vocabulary sizes\n",
        "x_voc_size = len(x_tokenizer.word_index) + 1\n",
        "y_voc_size = len(y_tokenizer.word_index) + 1\n",
        "\n",
        "# 7. For inference decoding\n",
        "target_word_index = y_tokenizer.word_index\n",
        "reverse_target_word_index = y_tokenizer.index_word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6t-s8j_QNm4",
        "outputId": "cb62fc63-f575-4b8a-d1c3-7b1ba2d21a4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 72.34996102883866\n",
            "Total Coverage of rare words: 18.91712398196372\n"
          ]
        }
      ],
      "source": [
        "thresh=4\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "\n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SofnBZk-SJeE",
        "outputId": "c8294148-f78a-4e4b-fa25-a74b10d1359a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary in X = 1420\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Assuming 'original_x_tr' and 'original_x_val' hold the original training and validation texts\n",
        "original_x_tr = df['text'].values  # Original texts as a NumPy array or list\n",
        "original_x_val = df['text'].values  # Same for validation\n",
        "\n",
        "# Initialize the tokenizer\n",
        "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt)\n",
        "x_tokenizer.fit_on_texts(list(original_x_tr))  # Use the original texts for fitting\n",
        "\n",
        "# Convert text sequences into integer sequences\n",
        "x_tr_seq = x_tokenizer.texts_to_sequences(original_x_tr)\n",
        "x_val_seq = x_tokenizer.texts_to_sequences(original_x_val)\n",
        "\n",
        "# Padding zero up to maximum length\n",
        "x_tr = pad_sequences(x_tr_seq, maxlen=max_len_text, padding='post')\n",
        "x_val = pad_sequences(x_val_seq, maxlen=max_len_text, padding='post')\n",
        "\n",
        "# Size of vocabulary (+1 for padding token)\n",
        "x_voc = x_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in X = {}\".format(x_voc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rmZy8eqNSjLR"
      },
      "outputs": [],
      "source": [
        "# Assume 'df' is your original DataFrame with a 'summary' column containing the raw text summaries\n",
        "original_y_tr = df['summary'].values  # Ensure this is run before any encoding\n",
        "\n",
        "# Now prepare the tokenizer for the summaries\n",
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(original_y_tr))  # Fit using the original text data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSpPbpBJSl5I",
        "outputId": "26ff7492-39db-4b84-f1d8-0de1c818900b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 87.76550552251486\n",
            "Total Coverage of rare words: 43.94314773764506\n"
          ]
        }
      ],
      "source": [
        "thresh=6\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "\n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xydsSHxXQSMQ",
        "outputId": "3e73fcb5-0391-4a1f-cd48-c487cfba4952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary in Y = 289\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'df' has a 'summary' column with raw text summaries\n",
        "x, y = df['text'], df['summary']\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=0.3, random_state=0, shuffle=True)\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_tr)  # or all_text_data if available\n",
        "\n",
        "# Convert to sequences\n",
        "x_tr = tokenizer.texts_to_sequences(x_tr)\n",
        "x_val = tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "# Pad the sequences\n",
        "max_text_len = 50  # or whatever your limit is\n",
        "x_tr = pad_sequences(x_tr, maxlen=max_text_len, padding='post')\n",
        "x_val = pad_sequences(x_val, maxlen=max_text_len, padding='post')\n",
        "\n",
        "# Tokenizer setup\n",
        "y_tokenizer = Tokenizer(num_words=tot_cnt - cnt)\n",
        "y_tokenizer.fit_on_texts(list(y_tr))  # Ensure y_tr is a list of raw text summaries\n",
        "\n",
        "# Convert text sequences into integer sequences\n",
        "y_tr_seq = y_tokenizer.texts_to_sequences(y_tr)\n",
        "y_val_seq = y_tokenizer.texts_to_sequences(y_val)\n",
        "\n",
        "# Padding zero up to maximum length\n",
        "y_tr = pad_sequences(y_tr_seq, maxlen=max_len_headline, padding='post')\n",
        "y_val = pad_sequences(y_val_seq, maxlen=max_len_headline, padding='post')\n",
        "\n",
        "# Size of vocabulary (+1 for padding token not accounted for in num_words)\n",
        "y_voc = y_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in Y = {}\".format(y_voc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SBwtVw9XQ79I"
      },
      "outputs": [],
      "source": [
        "ind=[]\n",
        "for i in range(len(y_tr)):\n",
        "    cnt=0\n",
        "    for j in y_tr[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_tr=np.delete(y_tr,ind, axis=0)\n",
        "x_tr=np.delete(x_tr,ind, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZX78wntrQ76i"
      },
      "outputs": [],
      "source": [
        "ind=[]\n",
        "for i in range(len(y_val)):\n",
        "    cnt=0\n",
        "    for j in y_val[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_val=np.delete(y_val,ind, axis=0)\n",
        "x_val=np.delete(x_val,ind, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /Users/anuraagreddy/anaconda3/envs/tensorflow/lib/python3.10/site-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Users/anuraagreddy/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/anuraagreddy/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /Users/anuraagreddy/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /Users/anuraagreddy/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow as tf\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        latent_dim = input_shape[0][-1]  # assuming encoder and decoder outputs have same hidden size\n",
        "\n",
        "        self.W_a = self.add_weight(\n",
        "            name=\"W_a\", shape=(latent_dim, latent_dim),\n",
        "            initializer=\"random_normal\", trainable=True)\n",
        "\n",
        "        self.U_a = self.add_weight(\n",
        "            name=\"U_a\", shape=(latent_dim, latent_dim),\n",
        "            initializer=\"random_normal\", trainable=True)\n",
        "\n",
        "        self.V_a = self.add_weight(\n",
        "            name=\"V_a\", shape=(latent_dim, 1),\n",
        "            initializer=\"random_normal\", trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        encoder_output, decoder_output = inputs  # shapes: (batch, T_enc, dim), (batch, T_dec, dim)\n",
        "\n",
        "        decoder_time_steps = tf.shape(decoder_output)[1]\n",
        "        encoder_time_steps = tf.shape(encoder_output)[1]\n",
        "\n",
        "        # Expand dimensions to align encoder and decoder\n",
        "        dec_hidden_expanded = tf.expand_dims(decoder_output, 2)  # (batch, T_dec, 1, dim)\n",
        "        dec_hidden_tiled = tf.tile(dec_hidden_expanded, [1, 1, encoder_time_steps, 1])  # (batch, T_dec, T_enc, dim)\n",
        "\n",
        "        enc_out_expanded = tf.expand_dims(encoder_output, 1)  # (batch, 1, T_enc, dim)\n",
        "        enc_out_tiled = tf.tile(enc_out_expanded, [1, decoder_time_steps, 1, 1])  # (batch, T_dec, T_enc, dim)\n",
        "\n",
        "        # Attention mechanism\n",
        "        W_enc = tf.tensordot(enc_out_tiled, self.W_a, axes=[[3], [0]])\n",
        "        U_dec = tf.tensordot(dec_hidden_tiled, self.U_a, axes=[[3], [0]])\n",
        "        e = tf.nn.tanh(W_enc + U_dec)\n",
        "        score = tf.tensordot(e, self.V_a, axes=[[3], [0]])\n",
        "        score = tf.squeeze(score, axis=-1)  # (batch, T_dec, T_enc)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(score, axis=-1)  # (batch, T_dec, T_enc)\n",
        "\n",
        "        # Compute context vector\n",
        "        context_vector = tf.matmul(attention_weights, encoder_output)  # (batch, T_dec, dim)\n",
        "\n",
        "        return context_vector, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szV6Ic2jQ731",
        "outputId": "833d4740-7afa-43ee-aa48-5c4361e12308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary from the w2v model = 1420\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">284,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">601,200</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">721,200</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">57,800</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">721,200</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">601,200</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>),      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention_layer_fi… │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">180,300</span> │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)]   │            │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ attention_layer_… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense_final │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">289</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">173,689</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m200\u001b[0m)   │    \u001b[38;5;34m284,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m300\u001b[0m), │    \u001b[38;5;34m601,200\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m300\u001b[0m), │    \u001b[38;5;34m721,200\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m200\u001b[0m)   │     \u001b[38;5;34m57,800\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m300\u001b[0m), │    \u001b[38;5;34m721,200\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m300\u001b[0m), │    \u001b[38;5;34m601,200\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m),      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)]      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention_layer_fi… │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m300\u001b[0m), │    \u001b[38;5;34m180,300\u001b[0m │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│ (\u001b[38;5;33mAttentionLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m50\u001b[0m)]   │            │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m600\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ attention_layer_… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense_final │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m289\u001b[0m)   │    \u001b[38;5;34m173,689\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,340,589</span> (12.74 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,340,589\u001b[0m (12.74 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,340,589</span> (12.74 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,340,589\u001b[0m (12.74 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "import gensim\n",
        "from numpy import *\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"Size of vocabulary from the w2v model = {}\".format(x_voc))\n",
        "\n",
        "\n",
        "# Define constants\n",
        "latent_dim = 300\n",
        "embedding_dim = 200\n",
        "max_len_text = 50\n",
        "max_len_summary = 49\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_len_text,))\n",
        "enc_emb = Embedding(x_voc, embedding_dim, trainable=True)(encoder_inputs)\n",
        "\n",
        "encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "encoder_lstm3 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_len_summary,))\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "# ✅ Define attention layer\n",
        "attn_layer = AttentionLayer(name=\"attention_layer_final\")\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# Concatenate attention output and decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attn_out])\n",
        "\n",
        "# Dense output layer\n",
        "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'), name=\"decoder_dense_final\")\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Final Model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SRfgmE9jQ7xm"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "cVPKPYt8Q7uj"
      },
      "outputs": [],
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6EUvaKn0f11X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-20 21:37:38.237767: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 11s/step - loss: 5.3415 - val_loss: 1.2199\n",
            "Epoch 2/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 1.2798 - val_loss: 1.4224\n",
            "Epoch 3/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 12s/step - loss: 1.3905 - val_loss: 1.1455\n",
            "Epoch 4/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 14s/step - loss: 1.2167 - val_loss: 1.0917\n",
            "Epoch 5/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 13s/step - loss: 1.1513 - val_loss: 1.0988\n"
          ]
        }
      ],
      "source": [
        "y_tr = np.array(y_tr)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "decoder_input_data = y_tr[:, :-1]  \n",
        "decoder_target_data = y_tr[:, 1:] \n",
        "decoder_target_data = decoder_target_data[..., np.newaxis]\n",
        "\n",
        "val_decoder_input = y_val[:, :-1]\n",
        "val_decoder_target = y_val[:, 1:]\n",
        "val_decoder_target = val_decoder_target[..., np.newaxis]\n",
        "\n",
        "training_history = model.fit(\n",
        "    [x_tr, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    epochs=5,\n",
        "    batch_size=128,\n",
        "    validation_data=(\n",
        "        [x_val, val_decoder_input],\n",
        "        val_decoder_target\n",
        "    ),\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "# Inference inputs\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_inputs_single = Input(shape=(1,))\n",
        "decoder_embedding_inf = dec_emb_layer(decoder_inputs_single)\n",
        "\n",
        "decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(\n",
        "    decoder_embedding_inf, initial_state=decoder_hidden_inputs)\n",
        "\n",
        "# Attention context (reuse encoder_outputs at inference time)\n",
        "encoder_inf_input = Input(shape=(max_len_text, latent_dim))\n",
        "attn_out_inf, attn_states_inf = attn_layer([encoder_inf_input, decoder_outputs_inf])\n",
        "decoder_concat_inf = Concatenate(axis=-1)([decoder_outputs_inf, attn_out_inf])\n",
        "\n",
        "decoder_outputs_inf = decoder_dense(decoder_concat_inf)\n",
        "\n",
        "decoder_model = Model(\n",
        "    inputs=[decoder_inputs_single, encoder_inf_input] + decoder_hidden_inputs,\n",
        "    outputs=[decoder_outputs_inf, state_h_inf, state_c_inf]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "EdsgkFwATAyV"
      },
      "outputs": [],
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "yOLYbsvlWKRF"
      },
      "outputs": [],
      "source": [
        "def translate_sequence(input_seq):\n",
        "    # Fetch initial states from encoder output\n",
        "    initial_states = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Initialize the sequence with the start token\n",
        "    start_sequence = np.zeros((1, 1))\n",
        "    if 'sostok' in target_word_index:\n",
        "        start_sequence[0, 0] = target_word_index['sostok']\n",
        "    else:\n",
        "        print(\"Error: Start token 'sostok' not in vocabulary index.\")\n",
        "        return \"\"\n",
        "\n",
        "    translation = ''\n",
        "    end_of_sequence = False\n",
        "    while not end_of_sequence:\n",
        "        # Predict the next word's token and update states\n",
        "        predicted_tokens, state_h, state_c = decoder_model.predict([start_sequence] + initial_states)\n",
        "\n",
        "        # Choose the token with highest probability\n",
        "        next_token_index = np.argmax(predicted_tokens[0, -1, :])\n",
        "        next_word = reverse_target_word_index.get(next_token_index, '')\n",
        "\n",
        "        # Check if the translation should end\n",
        "        if next_word == '' or len(translation) > max_decoder_seq_length:\n",
        "            end_of_sequence = True\n",
        "        else:\n",
        "            translation += ' ' + next_word\n",
        "\n",
        "            # Prepare the next token for the LSTM input\n",
        "            start_sequence = np.zeros((1, 1))\n",
        "            start_sequence[0, 0] = next_token_index\n",
        "\n",
        "            # Update LSTM states\n",
        "            initial_states = [state_h, state_c]\n",
        "\n",
        "    return translation.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Q7qVYiPfaE79"
      },
      "outputs": [],
      "source": [
        "# Check if 'sostok' is not in the dictionary and add it\n",
        "if 'sostok' not in target_word_index:\n",
        "    target_word_index['sostok'] = len(target_word_index) + 1  # Assign a new unique index\n",
        "\n",
        "# Do the same for 'eostok' if necessary\n",
        "if 'eostok' not in target_word_index:\n",
        "    target_word_index['eostok'] = len(target_word_index) + 1  # Assign a new unique index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "sA8Fqs8IWNUV"
      },
      "outputs": [],
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if i != 0 and i not in [target_word_index.get('sostok', None), target_word_index.get('eostok', None)]:\n",
        "            newString += reverse_target_word_index.get(i, '') + ' '\n",
        "    return newString.strip()\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if i != 0:\n",
        "            newString += reverse_source_word_index.get(i, '') + ' '\n",
        "    return newString.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding\n",
        "\n",
        "# 1. Fresh input layer for inference\n",
        "encoder_inputs_inf = Input(shape=(max_len_text,), name=\"encoder_inputs_inf\")\n",
        "\n",
        "# 2. Reuse layers used during training\n",
        "enc_emb_inf = Embedding(x_voc_size, embedding_dim, trainable=True)(encoder_inputs_inf)\n",
        "\n",
        "# 3. Reuse the same LSTM layers from training\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb_inf)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "encoder_outputs_inf, state_h, state_c = encoder_lstm3(encoder_output2)\n",
        "\n",
        "# 4. Final inference encoder model\n",
        "encoder_model = Model(inputs=encoder_inputs_inf, outputs=[encoder_outputs_inf, state_h, state_c])\n",
        "\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs_single)\n",
        "# LSTM step\n",
        "decoder_outputs2, state_h, state_c = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "attn_out_inf, _ = attn_layer([encoder_inf_input, decoder_outputs2])  # enc_out must be (None, 49, latent_dim)\n",
        "decoder_concat_input_inf = Concatenate(axis=-1)([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# Attention\n",
        "attn_out_inf, attn_states_inf = attn_layer([encoder_inf_input, decoder_outputs2])\n",
        "decoder_concat_input_inf = Concatenate(axis=-1)([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# Final prediction layer\n",
        "decoder_outputs2 = decoder_dense(decoder_concat_input_inf)\n",
        "\n",
        "# Now define model with 4 inputs\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs_single, encoder_inf_input, decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2, state_h2, state_c2]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Decoder Inputs\n",
        "decoder_inputs_single = Input(shape=(1,), name=\"decoder_word_input_inf\")  # one word at a time\n",
        "encoder_inf_input = Input(shape=(max_len_text, latent_dim), name=\"encoder_output_inf\")\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,), name=\"decoder_h_input\")\n",
        "decoder_state_input_c = Input(shape=(latent_dim,), name=\"decoder_c_input\")\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# Embedding for one word\n",
        "dec_emb_inf = dec_emb_layer(decoder_inputs_single)\n",
        "\n",
        "# Decoder LSTM output for one timestep\n",
        "decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(\n",
        "    dec_emb_inf, initial_state=decoder_states_inputs\n",
        ")\n",
        "\n",
        "# Attention output for decoder step\n",
        "attn_out_inf, attn_states_inf = attn_layer([encoder_inf_input, decoder_outputs_inf])\n",
        "decoder_concat_input = Concatenate(axis=-1)([decoder_outputs_inf, attn_out_inf])\n",
        "\n",
        "# Final Dense output\n",
        "decoder_outputs_inf = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define inference decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs_single, encoder_inf_input] + decoder_states_inputs,\n",
        "    [decoder_outputs_inf, state_h_inf, state_c_inf]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Ensure input is numpy array\n",
        "    input_seq = np.array(input_seq, dtype=np.int32)\n",
        "\n",
        "    # Encode input sequence\n",
        "    enc_out, state_h, state_c = encoder_model.predict(input_seq)\n",
        "    states_value = [state_h, state_c]\n",
        "\n",
        "    # Start with the <starttoken>\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = target_word_index['starttoken']  # Use correct key\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq, enc_out] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = reverse_target_word_index.get(sampled_token_index, '')\n",
        "\n",
        "        if (sampled_word == 'endtoken' or len(decoded_sentence.split()) >= max_len_summary):\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        # Update target sequence and states\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "-oq5w5jUg-90"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Input 0 of layer \"functional_3\" is incompatible with the layer: expected shape=(None, 50), found shape=(1, 48)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[59], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m original_summary \u001b[38;5;241m=\u001b[39m seq2summary(y_tr[i])\n\u001b[1;32m     11\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m x_tr[i]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, x_tr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])  \n\u001b[0;32m---> 12\u001b[0m predicted_summary \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Append each set of results to the data list as a dictionary\u001b[39;00m\n\u001b[1;32m     15\u001b[0m data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m\"\u001b[39m: review,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Summary\u001b[39m\u001b[38;5;124m\"\u001b[39m: original_summary,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Summary\u001b[39m\u001b[38;5;124m\"\u001b[39m: predicted_summary\n\u001b[1;32m     19\u001b[0m })\n",
            "Cell \u001b[0;32mIn[57], line 6\u001b[0m, in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m      3\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(input_seq, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Encode input sequence\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m enc_out, state_h, state_c \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m states_value \u001b[38;5;241m=\u001b[39m [state_h, state_c]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Start with the <starttoken>\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/src/layers/input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"functional_3\" is incompatible with the layer: expected shape=(None, 50), found shape=(1, 48)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize a list to hold dictionaries for each set of data\n",
        "data = []\n",
        "\n",
        "# Process each sequence\n",
        "for i in range(0, 1):\n",
        "    review = seq2text(x_tr[i])\n",
        "    original_summary = seq2summary(y_tr[i])\n",
        "    \n",
        "    input_seq = x_tr[i].reshape(1, x_tr.shape[1])  \n",
        "    predicted_summary = decode_sequence(input_seq)\n",
        "\n",
        "    # Append each set of results to the data list as a dictionary\n",
        "    data.append({\n",
        "        \"Review\": review,\n",
        "        \"Original Summary\": original_summary,\n",
        "        \"Predicted Summary\": predicted_summary\n",
        "    })\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Specify the file path for the CSV\n",
        "file_path = \"/Users/anuraagreddy/Desktop/MS\\\\(UIC\\\\)/Spring '25/CS 521 \\\\(SNLP\\\\)/Stat NLP Project/CS521_TextSummarization_Project/LSTM_Prediction_Results.csv\"\n",
        "\n",
        "# Write the DataFrame to a CSV file\n",
        "df.to_csv(file_path, index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
